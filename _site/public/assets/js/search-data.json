{"0": {
    "doc": "About OpenSearch",
    "title": "Introduction to OpenSearch",
    "content": "OpenSearch is a distributed search and analytics engine based on Apache Lucene. After adding your data to OpenSearch, you can perform full-text searches on it with all of the features you might expect: search by field, search multiple indices, boost fields, rank results by score, sort results by field, and aggregate results. Unsurprisingly, people often use search engines like OpenSearch as the backend for a search application—think Wikipedia or an online store. It offers excellent performance and can scale up and down as the needs of the application grow or shrink. An equally popular, but less obvious use case is log analytics, in which you take the logs from an application, feed them into OpenSearch, and use the rich search and visualization functionality to identify issues. For example, a malfunctioning web server might throw a 500 error 0.5% of the time, which can be hard to notice unless you have a real-time graph of all HTTP status codes that the server has thrown in the past four hours. You can use OpenSearch Dashboards to build these sorts of visualizations from data in OpenSearch. ",
    "url": "http://localhost:4000/about/#introduction-to-opensearch",
    "relUrl": "/about/#introduction-to-opensearch"
  },"1": {
    "doc": "About OpenSearch",
    "title": "Clusters and nodes",
    "content": "Its distributed design means that you interact with OpenSearch clusters. Each cluster is a collection of one or more nodes, servers that store your data and process search requests. You can run OpenSearch locally on a laptop—its system requirements are minimal—but you can also scale a single cluster to hundreds of powerful machines in a data center. In a single node cluster, such as a laptop, one machine has to do everything: manage the state of the cluster, index and search data, and perform any preprocessing of data prior to indexing it. As a cluster grows, however, you can subdivide responsibilities. Nodes with fast disks and plenty of RAM might be great at indexing and searching data, whereas a node with plenty of CPU power and a tiny disk could manage cluster state. For more information on setting node types, see Cluster formation. ",
    "url": "http://localhost:4000/about/#clusters-and-nodes",
    "relUrl": "/about/#clusters-and-nodes"
  },"2": {
    "doc": "About OpenSearch",
    "title": "Indices and documents",
    "content": "OpenSearch organizes data into indices. Each index is a collection of JSON documents. If you have a set of raw encyclopedia articles or log lines that you want to add to OpenSearch, you must first convert them to JSON. A simple JSON document for a movie might look like this: . { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } . When you add the document to an index, OpenSearch adds some metadata, such as the unique document ID: . { \"_index\": \"&lt;index-name&gt;\", \"_type\": \"_doc\", \"_id\": \"&lt;document-id&gt;\", \"_version\": 1, \"_source\": { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } } . Indices also contain mappings and settings: . | A mapping is the collection of fields that documents in the index have. In this case, those fields are title and release_date. | Settings include data like the index name, creation date, and number of shards. | . ",
    "url": "http://localhost:4000/about/#indices-and-documents",
    "relUrl": "/about/#indices-and-documents"
  },"3": {
    "doc": "About OpenSearch",
    "title": "Primary and replica shards",
    "content": "OpenSearch splits indices into shards for even distribution across nodes in a cluster. For example, a 400 GB index might be too large for any single node in your cluster to handle, but split into ten shards, each one 40 GB, OpenSearch can distribute the shards across ten nodes and work with each shard individually. By default, OpenSearch creates a replica shard for each primary shard. If you split your index into ten shards, for example, OpenSearch also creates ten replica shards. These replica shards act as backups in the event of a node failure—OpenSearch distributes replica shards to different nodes than their corresponding primary shards—but they also improve the speed and rate at which the cluster can process search requests. You might specify more than one replica per index for a search-heavy workload. Despite being a piece of an OpenSearch index, each shard is actually a full Lucene index—confusing, we know. This detail is important, though, because each instance of Lucene is a running process that consumes CPU and memory. More shards is not necessarily better. Splitting a 400 GB index into 1,000 shards, for example, would place needless strain on your cluster. A good rule of thumb is to keep shard size between 10–50 GB. ",
    "url": "http://localhost:4000/about/#primary-and-replica-shards",
    "relUrl": "/about/#primary-and-replica-shards"
  },"4": {
    "doc": "About OpenSearch",
    "title": "REST API",
    "content": "You interact with OpenSearch clusters using the REST API, which offers a lot of flexibility. You can use clients like curl or any programming language that can send HTTP requests. To add a JSON document to an OpenSearch index (i.e. index a document), you send an HTTP request: . PUT https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_doc/&lt;document-id&gt; { \"title\": \"The Wind Rises\", \"release_date\": \"2013-07-20\" } . To run a search for the document: . GET https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_search?q=wind . To delete the document: . DELETE https://&lt;host&gt;:&lt;port&gt;/&lt;index-name&gt;/_doc/&lt;document-id&gt; . You can change most OpenSearch settings using the REST API, modify indices, check the health of the cluster, get statistics—almost everything. ",
    "url": "http://localhost:4000/about/#rest-api",
    "relUrl": "/about/#rest-api"
  },"5": {
    "doc": "About OpenSearch",
    "title": "About OpenSearch",
    "content": " ",
    "url": "http://localhost:4000/about/",
    "relUrl": "/about/"
  },"6": {
    "doc": "Breaking changes",
    "title": "2.0.0",
    "content": "Remove mapping types parameter . The type parameter has been removed from all OpenSearch API endpoints. Instead, indexes can be categorized by document type. For more details, see issue #1940. Deprecate outdated nomenclature . In order for OpenSearch to include more inclusive naming conventions, we’ve replaced the following terms in our code with more inclusive terms: . | “Whitelist” is now “Allow list” | “Blacklist” is now “Deny list” | “Master” is now “Cluster Manager” | . If you are still using the outdated terms in the context of the security APIs or for node management, your calls and automation will continue to work until the terms are removed later in 2022. Add OpenSearch Notifications plugins . In OpenSearch 2.0, the Alerting plugin is now integrated with new plugins for Notifications. If you want to continue to use the notification action in the Alerting plugin, install the new backend plugins notifications-core and notifications. If you want to manage notifications in OpenSearch Dashboards, use the new notificationsDashboards plugin. For more information, see Questions about destinations on the Monitors page. Drop support for JDK 8 . A Lucene upgrade forced OpenSearch to drop support for JDK 8. As a consequence, the Java high-level REST client no longer supports JDK 8. Restoring JDK 8 support is currently an opensearch-java proposal #156 and will require removing OpenSearch core as a dependency from the Java client (issue #262). ",
    "url": "http://localhost:4000/breaking-changes/#200",
    "relUrl": "/breaking-changes/#200"
  },"7": {
    "doc": "Breaking changes",
    "title": "Breaking changes",
    "content": " ",
    "url": "http://localhost:4000/breaking-changes/",
    "relUrl": "/breaking-changes/"
  },"8": {
    "doc": "OpenSearch documentation",
    "title": "OpenSearch documentation",
    "content": "Welcome to the OpenSearch documentation! With this documentation, you’ll learn how to use OpenSearch — the only 100% open-source search, analytics, and visualization suite. We have a dedicated and growing number of technical writers who are building our documentation library. We also welcome and encourage community input. To contribute, see the Contributing file. A good place to start is by browsing issues labeled “good first issue.” . ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"9": {
    "doc": "OpenSearch documentation",
    "title": "test for right nav",
    "content": "hfbvhs vseh visev eisv iwe ve viwe vihw evih sjbsz fskhb vfg weuv wes fcug esvugesuigvf ugsd fjweuov uwe vu suov ouwev suw eov wueosv sjbe vuose v a . ",
    "url": "http://localhost:4000/#test-for-right-nav",
    "relUrl": "/#test-for-right-nav"
  },"10": {
    "doc": "OpenSearch documentation",
    "title": "Getting started",
    "content": ". | About OpenSearch | Quickstart | Install OpenSearch | Install OpenSearch Dashboards | See the FAQ | . ",
    "url": "http://localhost:4000/#getting-started",
    "relUrl": "/#getting-started"
  },"11": {
    "doc": "OpenSearch documentation",
    "title": "Why use OpenSearch?",
    "content": "With OpenSearch, you can perform the following use cases: . | | | | . | Fast, Scalable Full-text Search | Application and Infrastructure Monitoring | Security and Event Information Management | Operational Health Tracking | . | Help users find the right information within your application, website, or data lake catalog. | Easily store and analyze log data, and set automated alerts for underperformance. | Centralize logs to enable real-time security monitoring and forensic analysis. | Use observability logs, metrics, and traces to monitor your applications and business in real time. | . Additional features and plugins: . OpenSearch has several features and plugins to help index, secure, monitor, and analyze your data. Most OpenSearch plugins have corresponding OpenSearch Dashboards plugins that provide a convenient, unified user interface. | Anomaly detection - Identify atypical data and receive automatic notifications | KNN - Find “nearest neighbors” in your vector data | Performance Analyzer - Monitor and optimize your cluster | SQL - Use SQL or a piped processing language to query your data | Index State Management - Automate index operations | ML Commons plugin - Train and execute machine-learning models | Asynchronous search - Run search requests in the background | Cross-cluster replication - Replicate your data across multiple OpenSearch clusters | . ",
    "url": "http://localhost:4000/#why-use-opensearch",
    "relUrl": "/#why-use-opensearch"
  },"12": {
    "doc": "OpenSearch documentation",
    "title": "The secure path forward",
    "content": "OpenSearch includes a demo configuration so that you can get up and running quickly, but before using OpenSearch in a production environment, you must configure the security plugin manually with your own certificates, authentication method, users, and passwords. ",
    "url": "http://localhost:4000/#the-secure-path-forward",
    "relUrl": "/#the-secure-path-forward"
  },"13": {
    "doc": "OpenSearch documentation",
    "title": "Looking for the Javadoc?",
    "content": "See opensearch.org/javadocs/. ",
    "url": "http://localhost:4000/#looking-for-the-javadoc",
    "relUrl": "/#looking-for-the-javadoc"
  },"14": {
    "doc": "OpenSearch documentation",
    "title": "Get involved",
    "content": "OpenSearch is supported by Amazon Web Services. All components are available under the Apache License, Version 2.0 on GitHub. The project welcomes GitHub issues, bug fixes, features, plugins, documentation—anything at all. To get involved, see Contributing on the OpenSearch website. OpenSearch includes certain Apache-licensed Elasticsearch code from Elasticsearch B.V. and other source code. Elasticsearch B.V. is not the source of that other source code. ELASTICSEARCH is a registered trademark of Elasticsearch B.V. ",
    "url": "http://localhost:4000/#get-involved",
    "relUrl": "/#get-involved"
  },"15": {
    "doc": "Quickstart",
    "title": "Quickstart",
    "content": "Get started using OpenSearch and OpenSearch Dashboards by deploying your containers with Docker. Before proceeding, you need to get Docker and Docker Compose installed on your local machine. The Docker Compose commands used in this guide are written with a hyphen (for example, docker-compose). If you installed Docker Desktop on your machine, which automatically installs a bundled version of Docker Compose, then you should remove the hyphen. For example, change docker-compose to docker compose. ",
    "url": "http://localhost:4000/quickstart/",
    "relUrl": "/quickstart/"
  },"16": {
    "doc": "Quickstart",
    "title": "Starting your cluster",
    "content": "You’ll need a special file, called a Compose file, that Docker Compose uses to define and create the containers in your cluster. The OpenSearch Project provides a sample Compose file that you can use to get started. Learn more about working with Compose files by reviewing the official Compose specification. | Before running OpenSearch on your machine, you should disable memory paging and swapping performance on the host to improve performance and increase the number of memory maps available to OpenSearch. See important system settings for more information. # Disable memory paging and swapping. sudo swapoff -a # Edit the sysctl config file that defines the host's max map count. sudo vi /etc/sysctl.conf # Set max map count to the recommended value of 262144. vm.max_map_count=262144 # Reload the kernel parameters. sudo sysctl -p . | Download the sample Compose file to your host. You can download the file with command line utilities like curl and wget, or you can manually copy docker-compose.yml from the OpenSearch Project documentation-website repository using a web browser. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/docker-compose.yml # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/docker-compose.yml . | In your terminal application, navigate to the directory containing the docker-compose.yml file you just downloaded, and run the following command to create and start the cluster as a background process. docker-compose up -d . | Confirm that the containers are running with the command docker-compose ps. You should see an output like the following: $ docker-compose ps NAME COMMAND SERVICE STATUS PORTS opensearch-dashboards \"./opensearch-dashbo…\" opensearch-dashboards running 0.0.0.0:5601-&gt;5601/tcp opensearch-node1 \"./opensearch-docker…\" opensearch-node1 running 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp, 0.0.0.0:9600-&gt;9600/tcp, 9650/tcp opensearch-node2 \"./opensearch-docker…\" opensearch-node2 running 9200/tcp, 9300/tcp, 9600/tcp, 9650/tcp . | Query the OpenSearch REST API to verify that the service is running. You should use -k (also written as --insecure) to disable host name checking because the default security configuration uses demo certificates. Use -u to pass the default username and password (admin:admin). curl https://localhost:9200 -ku admin:admin . Sample response: . { \"name\" : \"opensearch-node1\", \"cluster_name\" : \"opensearch-cluster\", \"cluster_uuid\" : \"Cd7SL5ysRSyuau325M3h9w\", \"version\" : { \"distribution\" : \"opensearch\", \"number\" : \"2.3.0\", \"build_type\" : \"tar\", \"build_hash\" : \"6f6e84ebc54af31a976f53af36a5c69d474a5140\", \"build_date\" : \"2022-09-09T00:07:12.137133581Z\", \"build_snapshot\" : false, \"lucene_version\" : \"9.3.0\", \"minimum_wire_compatibility_version\" : \"7.10.0\", \"minimum_index_compatibility_version\" : \"7.0.0\" }, \"tagline\" : \"The OpenSearch Project: https://opensearch.org/\" } . | Explore OpenSearch Dashboards by opening http://localhost:5601/ in a web browser on the same host that is running your OpenSearch cluster. The default username is admin and the default password is admin. | . ",
    "url": "http://localhost:4000/quickstart/#starting-your-cluster",
    "relUrl": "/quickstart/#starting-your-cluster"
  },"17": {
    "doc": "Quickstart",
    "title": "Create an index and field mappings using sample data",
    "content": "Create an index and define field mappings using a dataset provided by the OpenSearch Project. The same fictitious e-commerce data is also used for sample visualizations in OpenSearch Dashboards. To learn more, see Getting started with OpenSearch Dashboards. | Download ecommerce-field_mappings.json. This file defines a mapping for the sample data you will use. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce-field_mappings.json # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce-field_mappings.json . | Download ecommerce.json. This file contains the index data formatted so that it can be ingested by the bulk API. To learn more, see index data and Bulk. # Using cURL: curl -O https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce.json # Using wget: wget https://raw.githubusercontent.com/opensearch-project/documentation-website/2.5/assets/examples/ecommerce.json . | Define the field mappings with the mapping file. curl -H \"Content-Type: application/x-ndjson\" -X PUT \"https://localhost:9200/ecommerce\" -ku admin:admin --data-binary \"@ecommerce-field_mappings.json\" . | Upload the index to the bulk API. curl -H \"Content-Type: application/x-ndjson\" -X PUT \"https://localhost:9200/ecommerce/_bulk\" -ku admin:admin --data-binary \"@ecommerce.json\" . | Query the data using the search API. The following command submits a query that will return documents where customer_first_name is Sonya. curl -H 'Content-Type: application/json' -X GET \"https://localhost:9200/ecommerce/_search?pretty=true\" -ku admin:admin -d' {\"query\":{\"match\":{\"customer_first_name\":\"Sonya\"}}}' . Queries submitted to the OpenSearch REST API will generally return a flat JSON by default. For a human readable response body, use the query parameter pretty=true. For more information about pretty and other useful query parameters, see Common REST parameters. | Access OpenSearch Dashboards by opening http://localhost:5601/ in a web browser on the same host that is running your OpenSearch cluster. The default username is admin and the default password is admin. | On the top menu bar, go to Management &gt; Dev Tools. | In the left pane of the console, enter the following: GET ecommerce/_search { \"query\": { \"match\": { \"customer_first_name\": \"Sonya\" } } } . | Choose the triangle icon at the top right of the request to submit the query. You can also submit the request by pressing Ctrl+Enter (or Cmd+Enter for Mac users). To learn more about using the OpenSearch Dashboards console for submitting queries, see Running queries in the console. | . ",
    "url": "http://localhost:4000/quickstart/#create-an-index-and-field-mappings-using-sample-data",
    "relUrl": "/quickstart/#create-an-index-and-field-mappings-using-sample-data"
  },"18": {
    "doc": "Quickstart",
    "title": "Next steps",
    "content": "You successfully deployed your own OpenSearch cluster with OpenSearch Dashboards and added some sample data. Now you’re ready to learn about configuration and functionality in more detail. Here are a few recommendations on where to begin: . | About the security plugin | OpenSearch configuration | OpenSearch plugin installation | Getting started with OpenSearch Dashboards | OpenSearch tools | Index APIs | . ",
    "url": "http://localhost:4000/quickstart/#next-steps",
    "relUrl": "/quickstart/#next-steps"
  },"19": {
    "doc": "Quickstart",
    "title": "Common issues",
    "content": "Review these common issues and suggested solutions if your containers fail to start or exit unexpectedly. Docker commands require elevated permissions . Eliminate the need for running your Docker commands with sudo by adding your user to the docker user group. See Docker’s Post-installation steps for Linux for more information. sudo usermod -aG docker $USER . Error message: “-bash: docker-compose: command not found” . If you installed Docker Desktop, then Docker Compose is already installed on your machine. Try docker compose (without the hyphen) instead of docker-compose. See Use Docker Compose. Error message: “docker: ‘compose’ is not a docker command.” . If you installed Docker Engine, then you must install Docker Compose separately, and you will use the command docker-compose (with a hyphen). See Docker Compose. Error message: “max virtual memory areas vm.max_map_count [65530] is too low” . OpenSearch will fail to start if your host’s vm.max_map_count is too low. Review the important system settings if you see the following errors in the service log, and set vm.max_map_count appropriately. opensearch-node1 | ERROR: [1] bootstrap checks failed opensearch-node1 | [1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] opensearch-node1 | ERROR: OpenSearch did not exit normally - check the logs at /usr/share/opensearch/logs/opensearch-cluster.log . ",
    "url": "http://localhost:4000/quickstart/#common-issues",
    "relUrl": "/quickstart/#common-issues"
  },"20": {
    "doc": "Version history",
    "title": "Version history",
    "content": "| OpenSearch version | Release highlights | Release date | . | 2.5.0 | Includes index management UI enhancements, multi-layer maps, Jaeger support for observability, Debian distributions, returning cluster health by awareness attribute, cluster manager task throttling, weighted zonal search request routing policy, and query string support in index rollups. Experimental features include request-level durability in remote-backed storage and GPU acceleration for ML nodes. For a full list of release highlights, see the Release Notes. | 24 January 2023 | . | 2.4.1 | Includes maintenance changes and bug fixes for gradle check and indexing pressure tests. Adds support for skipping changelog. | 13 December 2022 | . | 2.4.0 | Includes Windows support, Point-in-time search, custom k-NN filtering, xy_point and xy_shape field types for Cartesian coordinates, GeoHex grid aggregation, and resilience enhancements, including search backpressure. In OpenSearch Dashboards, this release adds snapshot restore functionality, multiple authentication, and aggregate view of saved objects. This release includes the following experimental features: searchable snapshots, Compare Search Results, multiple data sources in OpenSearch Dashboards, a new Model Serving Framework in ML Commons, a new Neural Search plugin that supports semantic search, and a new Security Analytics plugin to analyze security logs. For a full list of release highlights, see the Release Notes. | 15 November 2022 | . | 2.3.0 | This release includes the following experimental features: segment replication, remote-backed storage, and drag and drop for OpenSearch Dashboards. Experimental features allow you to test new functionality in OpenSearch. Because these features are still being developed, your testing and feedback can help shape the development of the feature before it’s official released. We do not recommend use of experimental features in production. Additionally, this release adds maketime and makedate datetime functions for the SQL plugin. Creates a new OpenSearch Playground demo site for OpenSearch Dashboards. For a full list of release highlights, see the Release Notes. | 14 September 2022 | . | 2.2.1 | Includes gradle updates and bug fixes for gradle check. | 01 September 2022 | . | 2.2.0 | Includes support for Logistic Regression and RCFSummarize machine learning algorithms in ML Commons, Lucene or C-based Nmslib and Faiss libraries for approximate k-NN search, search by relevance using SQL and PPL queries, custom region maps for visualizations, and rollup enhancements. For a full list of release highlights, see the Release Notes. | 11 August 2022 | . | 2.1.0 | Includes support for dedicated ML node in the ML Commons plugin, relevance search and other features in SQL, multi-terms aggregation, and Snapshot Management. For a full list of release highlights, see the Release Notes. | 07 July 2022 | . | 2.0.1 | Includes bug fixes and maintenance updates for Alerting and Anomaly Detection. | 16 June 2022 | . | 2.0.0 | Includes document-level monitors for alerting, OpenSearch Notifications plugins, and Geo Map Tiles in OpenSearch Dashboards. Also adds support for Lucene 9 and bug fixes for all OpenSearch plugins. For a full list of release highlights, see the Release Notes. | 26 May 2022 | . | 2.0.0-rc1 | The Release Candidate for 2.0.0. This version allows you to preview the upcoming 2.0.0 release before the GA release. The preview release adds document-level alerting, support for Lucene 9, and the ability to use term lookup queries in document level security. | 03 May 2022 | . | 1.3.7 | Adds Windows support. Includes maintenance updates and bug fixes for error handling. | 13 December 2022 | . | 1.3.6 | Includes maintenance updates and bug fixes for tenancy in the OpenSearch Security Dashboards plugin. | 06 October 2022 | . | 1.3.5 | Includes maintenance updates and bug fixes for gradle check and OpenSearch security. | 01 September 2022 | . | 1.3.4 | Includes maintenance updates and bug fixes for OpenSearch and OpenSearch Dashboards. | 14 July 2022 | . | 1.3.3 | Adds enhancements to Anomaly Detection and ML Commons. Bug fixes for Anomaly Detection, Observability, and k-NN. | 09 June 2022 | . | 1.3.2 | Bug fixes for Anomaly Detection and the Security Dashboards Plugin, adds the option to install OpenSearch using RPM, as well as enhancements to the ML Commons execute task, and the removal of the job-scheduler zip in Anomaly Detection. | 05 May 2022 | . | 1.3.1 | Bug fixes when using document-level security, and adjusted ML Commons to use the latest RCF jar and protostuff to RCF model serialization. | 30 March 2022 | . | 1.3.0 | Adds Model Type Validation to Validate Detector API, continuous transforms, custom actions, applied policy parameter to Explain API, default action retries, and new rollover and transition conditions to Index Management, new ML Commons plugin, parse command to SQL, Application Analytics, Live Tail, Correlation, and Events Flyout to Observability, and auto backport and support for OPENSEARCH_JAVA_HOME to Performance Analyzer. Bug fixes. | 17 March 2022 | . | 1.2.4 | Updates Performance Analyzer, SQL, and Security plugins to Log4j 2.17.1, Alerting and Job Scheduler to cron-utils 9.1.6, and gson in Anomaly Detection and SQL. | 18 January 2022 | . | 1.2.3 | Updates the version of Log4j used in OpenSearch to Log4j 2.17.0 as recommended by the advisory in CVE-2021-45105. | 22 December 2021 | . | 1.2.0 | Adds observability, new validation API for Anomaly Detection, shard-level indexing back-pressure, new “match” query type for SQL and PPL, support for Faiss libraries in k-NN, and custom Dashboards branding. | 23 November 2021 | . | 1.1.0 | Adds cross-cluster replication, security for Index Management, bucket-level alerting, a CLI to help with upgrading from Elasticsearch OSS to OpenSearch, and enhancements to high cardinality data in the anomaly detection plugin. | 05 October 2021 | . | 1.0.1 | Bug fixes. | 01 September 2021 | . | 1.0.0 | General availability release. Adds compatibility setting for clients that require a version check before connecting. | 12 July 2021 | . | 1.0.0-rc1 | First release candidate. | 07 June 2021 | . | 1.0.0-beta1 | Initial beta release. Refactors plugins to work with OpenSearch. | 13 May 2021 | . ",
    "url": "http://localhost:4000/version-history/",
    "relUrl": "/version-history/"
  },"21": {
    "doc": "Overview",
    "title": "Overview",
    "content": "## Overview This document contains a list of maintainers in this repo. See [opensearch-project/.github/RESPONSIBILITIES.md](https://github.com/opensearch-project/.github/blob/main/RESPONSIBILITIES.md#maintainer-responsibilities) that explains what the role of maintainer means, what maintainers do in this and other repos, and how they should be doing it. If you're interested in contributing, and becoming a maintainer, see [CONTRIBUTING](/CONTRIBUTING.md). ## Current Maintainers | Maintainer | GitHub ID | Affiliation | -------------- | --------------------------------------------- | ----------- | Heather Halter | [hdhalter](https://github.com/hdhalter) | Amazon | Nate Archer | [Naarcha-AWS](https://github.com/Naarcha-AWS) | Amazon | ",
    "url": "http://localhost:4000/MAINTAINERS/",
    "relUrl": "/MAINTAINERS/"
  },"22": {
    "doc": "API reference page template",
    "title": "API reference page template",
    "content": "# API reference page template This template provides the basic structure for creating OpenSearch API documentation. It includes the most important elements that should appear in the documentation and helpful suggestions to help support them. Depending on the intended purpose of the API, *some sections will be required while others may not be applicable*. ### A note on terminology ### Terminology for API parameters varies in the software industry, where two or even three names may be used to label the same type of parameter. For the sake of consistency, we use the following nomenclature for parameters in our API documentation: * *Path parameter* – \"path parameter\" and \"URL parameter\" are sometimes used synonymously. To avoid confusion, we use \"path parameter\" in this documentation. * *Query parameter* – This parameter name is often used synonymously with \"request parameter.\" We use \"query parameter\" to be consistent. ### General usage for code elements When you describe any code element in a sentence, such as an API, a parameter, or a field, you can use the noun name. *Example usage*: The time field provides a timestamp for job completion. When you provide an exact example with a value, you can use the code element in code font. *Example usage*: The response provides a value for `time_field`, such as “timestamp.” Provide a REST API call example in `json` format. Optionally, also include the `curl` command if the call can only be executed in a command line. ## Basic elements for documentation The following sections describe the basic API documentation structure. Each section is discussed under its respective heading below. You can include only those elements appropriate to the API. Depending on where the documentation appears within a section or subsection, heading levels may be adjusted to fit with other content. 1. Name of API (heading level 2) 1. (Optional) Path and HTTP methods (heading level 3) 1. Path parameters (heading level 3) 1. Query parameters (heading level 3) 1. Request fields (heading level 3) 1. Sample request (heading level 4) 1. Sample response (heading level 4) 1. Response fields (heading level 3) ## API name Provide an API name that describes its function, followed by a description of its top use case and any usage recommendations. *Example function*: \"Autocomplete queries\" Use sentence capitalization for the heading (for example, \"Create or update mappings\"). When you refer to the API operation, you can use lowercase with code font. If there is a corresponding OpenSearch Dashboards feature, provide a “See also” link that references it. *Example*: “To learn more about monitor findings, see [Document findings](https://opensearch.org/docs/latest/monitoring-plugins/alerting/monitors/#document-findings).\" If applicable, provide any caveats to its usage with a note or tip, as in the following example: \"If you use the Security plugin, make sure you have the appropriate permissions.\" (To set this point in note-style format, follow the text on the next line with {: .note}) ### Path and HTTP methods For relatively complex API calls that include path parameters, it's sometimes a good idea to provide an example so that users can visualize how the request is properly formed. This section is optional and includes examples that illustrate how the endpoint and path parameters fit together in the request. The following is an example of this section for the nodes stats API: ```json GET /_nodes/stats GET /_nodes//stats GET /_nodes/stats/ GET /_nodes//stats/ GET /_nodes/stats// GET /_nodes//stats// ``` ### Path parameters While the API endpoint states a point of entry to a resource, the path parameter acts on the resource that precedes it. Path parameters come after the resource name in the URL. ```json GET _search/scroll/ ``` In the example above, the resource is `scroll` and its path parameter is ``. Introduce what the path parameters can do at a high level. Provide a table with parameter names and descriptions. Include a table with the following columns: *Parameter* – Parameter name in plain font. *Data type* – Data type capitalized (such as Boolean, String, or Integer). *Description* – Sentence to describe the parameter function, default values or range of values, and any usage examples. Parameter | Data type | Description :--- | :--- | :--- ### Query parameters In terms of placement, query parameters are always appended to the end of the URL and located to the right of the operator \"?\". Query parameters serve the purpose of modifying information to be retrieved from the resource. ```json GET _cat/aliases?v ``` In the example above, the endpoint is `aliases` and its query parameter is `v` (provides verbose output). Include a paragraph that describes how to use the query parameters with an example in code font. Include the query parameter operator \"?\" to delineate query parameters from path parameters. For GET and DELETE APIs: Introduce what you can do with the optional parameters. Include a table with the same columns as the path parameter table. Parameter | Data type | Description :--- | :--- | :--- ### Request fields For PUT and POST APIs: Introduce what the request fields are allowed to provide in the body of the request. Include a table with these columns: *Field* – Field name in plain font. *Data type* – Data type capitalized (such as Boolean, String, or Integer). *Description* – Sentence to describe the field’s function, default values or range of values, and any usage examples. Field | Data type | Description :--- | :--- | :--- #### Sample request Provide a sentence that describes what is shown in the example, followed by a cut-and-paste-ready API request in JSON format. Make sure that you test the request yourself in the Dashboards Dev Tools console to make sure it works. See the examples below. The following request gets all the settings in your index: ```json GET /sample-index1/_settings ``` The following request copies all of your field mappings and settings from a source index to a destination index: ```json POST _reindex { \"source\":{ \"index\":\"sample-index-1\" }, \"dest\":{ \"index\":\"sample-index-2\" } } ``` #### Sample response Include a JSON example response to show what the API returns. See the examples below. The `GET /sample-index1/_settings` request returns the following response fields: ```json { \"sample-index1\": { \"settings\": { \"index\": { \"creation_date\": \"1622672553417\", \"number_of_shards\": \"1\", \"number_of_replicas\": \"1\", \"uuid\": \"GMEA0_TkSaamrnJSzNLzwg\", \"version\": { \"created\": \"135217827\", \"upgraded\": \"135238227\" }, \"provided_name\": \"sample-index1\" } } } } ``` The `POST _reindex` request returns the following response fields: ```json { \"took\" : 4, \"timed_out\" : false, \"total\" : 0, \"updated\" : 0, \"created\" : 0, \"deleted\" : 0, \"batches\" : 0, \"version_conflicts\" : 0, \"noops\" : 0, \"retries\" : { \"bulk\" : 0, \"search\" : 0 }, \"throttled_millis\" : 0, \"requests_per_second\" : -1.0, \"throttled_until_millis\" : 0, \"failures\" : [ ] } ``` ### Response fields For PUT and POST APIs: Define all allowable response fields that can be returned in the body of the response. Field | Data type | Description :--- | :--- | :--- ",
    "url": "http://localhost:4000/STYLE_API_TEMPLATE/",
    "relUrl": "/STYLE_API_TEMPLATE/"
  },"23": {
    "doc": "OpenSearch Project style guidelines",
    "title": "OpenSearch Project style guidelines",
    "content": "# OpenSearch Project style guidelines Welcome to the content style guide for the OpenSearch Project. This guide covers the style standards to be observed when creating OpenSearch content and will evolve as we implement best practices and lessons learned in order to best serve the community. Our content is generally edited in accordance with the _AWS Style Guide_, the [Microsoft Writing Style Guide](https://docs.microsoft.com/en-us/style-guide/welcome/), [The Chicago Manual of Style](https://www.chicagomanualofstyle.org/home.html), and [Merriam-Webster](https://www.merriam-webster.com/) (listed in order of precedence); however, we may deviate from these style guides in order to maintain consistency and accommodate the unique needs of the community. This is by no means an exhaustive list of style standards, and we value transparency, so we welcome contributions to our style standards and guidelines. If you have a question regarding our standards or adherence/non-adherence to the style guides or would like to make a contribution, please tag @natebower on GitHub. ## Naming conventions, voice, tone, and brand personality traits The following sections provide guidance on OpenSearch Project naming conventions, voice, tone, and brand personality traits. ### Naming conventions The following naming conventions should be observed in OpenSearch Project content: * Capitalize both words when referring to the *OpenSearch Project*. * *OpenSearch* is the name for the distributed search and analytics engine used by Amazon OpenSearch Service. * Amazon OpenSearch Service is a managed service that makes it easy to deploy, operate, and scale OpenSearch. Use the full name *Amazon OpenSearch Service* on first appearance. The abbreviated service name, *OpenSearch Service*, can be used for subsequent appearances. * OpenSearch Dashboards is the UI for OpenSearch. On first appearance, use the full name *OpenSearch Dashboards*. *Dashboards* can be used for subsequent appearances. * Refer to OpenSearch Project customers as *users*, and refer to the larger group of users as *the community*. #### Product names Capitalize product names. The OpenSearch Project has three products: OpenSearch, OpenSearch Dashboards, and Data Prepper. For example: * “To install *OpenSearch*, download the Docker image.” * “To access *OpenSearch Dashboards*, open your browser and navigate to http://localhost:5601/app/home.” * “*Data Prepper* contains the following components:” Capitalize the names of clients and tools. For example: * “The OpenSearch *Python* client provides a more natural syntax for interacting with your cluster.” * “The *Go* client retries requests for a maximum of three times by default.” * “The *OpenSearch Kubernetes Operator* is an open-source Kubernetes operator that helps automate the deployment and provisioning of OpenSearch and OpenSearch Dashboards in a containerized environment.” * “You can send events to *Logstash* from many different sources.” #### Features Features are the individual building blocks of user experiences, reflect the functionality of a product, and are shared across different experiences. For example, the SQL/PPL, reporting, notifications, alerting, and anomaly detection used for observability are the same SQL/PPL, reporting, notifications, alerting, and anomaly detection used for general analytics, security analytics, and search analytics. Components of the user experience such as navigation, credentials management, theming, etc. are also considered to be features. Use lowercase when referring to features, unless you are referring to a formally named feature that is specific to OpenSearch. For example: * “The Notifications plugin provides a central location for all of your *notifications* from OpenSearch plugins.” * “*Remote-backed storage* is an experimental feature. Therefore, we do not recommend the use of *remote-backed storage* in a production environment.” * “You can take and restore *snapshots* using the snapshot API.” * “You can use the *VisBuilder* visualization type in OpenSearch Dashboards to create data visualizations by using a drag-and-drop gesture.” (You can refer to VisBuilder alone or qualify the term with “visualization type”.) * “As of OpenSearch 2.4, the *model-serving framework* only supports text embedding models without GPU acceleration.” #### Plugin names A plugin is a feature or distinct component that extends the functionality of OpenSearch. For now, capitalize plugin names, but use *plugin* sparingly. The concept of plugins will become obsolete once we re-architect the product. For example: * “Interaction with the *ML Commons* plugin occurs through either the REST API or [ad](https://opensearch.org/docs/latest/search-plugins/sql/ppl/functions#ad) and [kmeans](https://opensearch.org/docs/latest/search-plugins/sql/ppl/functions#kmeans) Piped Processing Language (PPL) commands.” * “Use the *Neural Search* plugin to integrate ML language models into your search workloads.” ### Voice and tone Voice is the point of view or style of a writer. Voice can refer to active or passive but may also refer to verb tense (past, present, future, and so on). Tone is the emotional undercurrent (such as calm or angry) of the voice. We strive to speak to the community with a consistent voice and tone, as if a single writer writes all content. Writing with a common voice also helps to establish the OpenSearch Project identity and brand. #### Voice The voice of the OpenSearch Project is people oriented and focused on empowering the user directly. We use language that emphasizes what the user can do with OpenSearch rather than what tasks OpenSearch can perform. Whenever possible, use the active voice instead of the passive voice. The passive form is typically wordier and can often cause writers to obscure the details of the action. For example, change the agentless passive it is recommended to the more direct we recommend. Refer to the reader as you (second person), and refer to the OpenSearch Project as we (first person). If there are multiple authors for a blog post, you can use we to refer to the authors as individuals. For procedures or instructions, ensure that action is taken by the user (“Then you can stop the container...”) rather than the writer (“We also have to stop the container...”). Reserve the first-person plural for speaking as the OpenSearch Project, with recommendations, warnings, or explanations. In general, use the present tense. Use the future tense only when an event happens later than, not immediately after, the action under discussion. #### Tone The tone of the OpenSearch Project is conversational, welcoming, engaging, and open. The overall tone is knowledgeable but humble, informal but authoritative, informative but not dry, and friendly without being overly familiar. We talk to readers in their own words, never assuming that they understand how OpenSearch works. We use precise technical terms where appropriate, but we avoid technical jargon and insider lingo. We speak to readers in simple, plain, everyday language. Avoid excessive words, such as please. Be courteous but not wordy. Extra detail can often be moved elsewhere. Use humor with caution because it is subjective, can be easily misunderstood, and can potentially alienate your audience. ### Brand personality traits | Personality trait | Description | Guidance | :--------- | :------- | :------ | **Clear and precise** | The OpenSearch Project understands that our community works, develops, and builds in roles and organizations that require precise thinking and thorough documentation. We strive to use precise language—to clearly say what we mean without leaving ideas open to interpretation, to support our assertions with facts and figures, and to provide credible and current (third-party) references where called for. We communicate in plain, direct language that is easily understood. Complex concepts are introduced in a concise, unambiguous way that does not assume knowledge on the part of the reader. High-level content is supported by links to more in-depth or technical content that users can engage with at their convenience. | - Write with clarity and choose words carefully. Think about the audience and how they might interpret your assertions. - Be specific. Avoid estimates or general claims when exact data can be provided. - Support claims with data. If something is “faster” or “more accurate,” say how much. - When citing third-party references, include direct links. | **Transparent and open** | As an open-source project, we exchange information with the community in an accessible and transparent manner. We publish our product plans in the open on GitHub, share relevant and timely information related to the project through our forum and/or our blog, and engage in open dialogues related to product and feature development in the public sphere. Anyone can view our roadmap, raise a question or an issue, or participate in our community meetings. | - Tell a complete story. If you’re walking the reader through a solution or sharing news, don’t skip important information. - Be forthcoming. Communicate time-sensitive news and information in a thorough and timely manner. - If there’s something the reader needs to know, say it up front. Don’t “bury the lede.” | **Collaborative and supportive** | We’re part of a community that is here to help. We aim to be resourceful on behalf of the community and encourage others to do the same. To facilitate an open exchange of ideas, we provide forums through which the community can ask and answer one another’s questions. | - Use conversational language that welcomes and engages the audience. Have a dialogue. - Invite discussion and feedback. We have several mechanisms for open discussion, including requests for comment (RFCs), a [community forum](https://forum.opensearch.org/), and [community meetings](https://www.meetup.com/OpenSearch/). | **Trustworthy and personable** | We stay grounded in the facts and the data. We do not overstate what our products are capable of. We demonstrate our knowledge in a humble but authoritative way and reliably deliver what we promise. We provide mechanisms and support that allow the audience to explore our products for themselves, demonstrating that our actions consistently match our words. We speak to the community in a friendly, welcoming, judgment-free way so that our audience perceives us as being approachable. Our content is people oriented and focused on empowering the user directly. | - Claims and assertions should be grounded in facts and data and supported accordingly. - Do not exaggerate or overstate. Let the facts and results speak for themselves. - Encourage the audience to explore our products for themselves. Offer guidance to help them do so. - Write directly and conversationally. Have a dialogue with your audience. Imagine writing as if you’re speaking directly to the person for whom you’re creating content. - Write from the community, for the community. Anyone creating or consuming content about OpenSearch is a member of the same group, with shared interest in learning about and building better search and analytics solutions. - Use judgment-free language. Words like simple, easy, and just create a skill judgment that may not apply to everyone in the OpenSearch community. | **Inclusive and accessible** | As an open-source project, The OpenSearch Project is for everyone, and we are inclusive. We value the diversity of backgrounds and perspectives in the OpenSearch community and welcome feedback from any contributor, regardless of their experience level. We design and create content so that people with disabilities can perceive, navigate, and interact with it. This ensures that our documentation is available and useful for everyone and helps improve the general usability of content. We understand our community is international and our writing takes that into account. We use plain language that avoids idioms and metaphors that may not be clear to the broader community. | - Use inclusive language to connect with the diverse and global OpenSearch Project audience.- Be careful with our word choices. - Avoid [sensitive terms](https://github.com/opensearch-project/documentation-website/blob/main/STYLE_GUIDE.md#sensitive-terms). - Don't use [offensive terms](https://github.com/opensearch-project/documentation-website/blob/main/STYLE_GUIDE.md#offensive-terms). - Don't use ableist or sexist language or language that perpetuates racist structures or stereotypes. - Links: Use link text that adequately describes the target page. For example, use the title of the target page instead of “here” or “this link.” In most cases, a formal cross-reference (the title of the page you’re linking to) is the preferred style because it provides context and helps readers understand where they’re going when they choose the link. - Images: &nbsp;&nbsp;- Add introductory text that provides sufficient context for each image. &nbsp;&nbsp;- Add ALT text that describes the image for screen readers. - Procedures: Not everyone uses a mouse, so use device-independent verbs; for example, use “choose” instead of “click.” - Location: When you’re describing the location of something else in your content, such as an image or another section, use words such as “preceding,” “previous,” or “following” instead of “above” and “below.” ## Style guidelines The following guidelines should be observed in OpenSearch Project content. ### Acronyms Spell out acronyms the first time that you use them on a page and follow them with the acronym in parentheses. Use the format `spelled-out term (acronym)`. On subsequent use, use the acronym alone. Do not capitalize the spelled-out form of an acronym unless the spelled-out form is a proper noun or the community generally capitalizes it. In all cases, our usage should reflect the community’s usage. In general, spell out acronyms once on a page. However, you can spell them out more often for clarity. Make an acronym plural by adding an *s* to the end of it. Do not add an apostrophe. How an acronym is pronounced determines whether you use the article *an* or *a* before it. If it's pronounced with an initial vowel sound, use *an*. Otherwise, use *a*. If the first use of an acronym is in a heading, retain the acronym in the heading, and then write out the term in the following body text, followed by the acronym in parentheses. Don't spell out the term in the heading with the acronym included in parentheses. If the first use of the service name is in a title or heading, use the short form of the name in the heading, and then use the long form followed by the short form in parentheses in the following body text. In general, spell out abbreviations that end with *-bit* or *-byte*. Use abbreviations only with numbers in specific measurements. Always include a space between the number and unit. Abbreviations that are well known and don't need to be spelled out are *KB*, *MB*, *GB*, and *TB*. Some acronyms are better known than their spelled-out counterparts or might be used almost exclusively. These include industry standard protocols, markdown and programming languages, and common file formats. You don't need to spell out these acronyms. The following table lists acronyms that you don't need to spell out. | Acronym | Spelled-out term | :--------- | :------- | 3D | three-dimensional | API | application programming interface | ASCII | American Standard Code for Information Interchange | BASIC | Beginner's All-Purpose Symbolic Instruction Code | BM25 | Best Match 25 | CPU | central processing unit | DOS | disk operating system | FAQ | frequently asked questions | FTP | File Transfer Protocol | GIF | Graphics Interchange Format | HTML | hypertext markup language | HTTP | hypertext transfer protocol | HTTPS | hypertext transfer protocol secure | HTTP(s) | Use to refer to both protocols, HTTP and HTTPS. | I/O | input/output | ID | identifier | IP | Internet protocol | JPEG | Joint Photographic Experts Group | JSON | JavaScript Object Notation | NAT | network address translation | NGINX | engine x | PDF | Portable Document Format | RAM | random access memory | REST | Representational State Transfer | RGB | red-green-blue | ROM | read-only memory | SAML | Security Assertion Markup Language | SDK | software development kit | SSL | Secure Sockets Layer | TCP | Transmission Control Protocol | TIFF | Tagged Image File Format | TLS | Transport Layer Security | UI | user interface | URI | uniform resource identifier | URL | uniform resource locator | UTC | Coordinated Universal Time | UTF | Unicode Transformation Format | XML | Extensible Markup Language | YAML | YAML Ain't Markup Language | ### Formatting and organization - You can refer to APIs in three ways: 1. When referring to API names, capitalize all words in the name (example: \"Field Capabilities API\"). 2. When referring to API operations by the exact name of the endpoint, use lowercase with code format (example: \"`_field_caps` API\"). 3. When describing API operations but not using the exact name of the endpoint, use lowercase (example: \"field capabilities API operations\" or \"field capabilities operations\"). - The following guidelines apply to all list types: - Make lists parallel in content and structure. Don’t mix single words with phrases, don’t start some phrases with a noun and others with a verb, and don’t mix verb forms. - Present the items in alphabetical order if the order of items is arbitrary. - Capitalize the ﬁrst letter of the ﬁrst word of each list item. - If the list is simple, you don’t need end punctuation for the list items. - If the list has a mixture of phrases and sentences, punctuate each list item. - Punctuate each list item with a period if a list item has more than one sentence. - Punctuate list items consistently. If at least one item in a list requires a period, use a period for all items in that list. - Titles are optional for most lists. If used, the title comes after an introductory sentence. - Titles are highly recommended for procedures. Avoid titles for bulleted lists. - Introductory sentences are required for lists. - Introductory sentences should be complete sentences. - Introductory sentences should end with a period if the list has a title. - Introductory sentences should end with a colon if the list does not have a title. - Don’t use semicolons, commas, or conjunctions (like and or or) at the end of list items. - Stacked headings should never appear in our content. Stacked headings are any two consecutive headings without intervening text. Even if it is just an introductory sentence, there should always be text under any heading. - Use italics for the titles of books, periodicals, and reference guides. However, do not use italics when the title of a work is also a hyperlink. - Reference images in the text that precedes them. For example, \"..., as shown in the following image.\" ### Links - **Formal cross-references**: In most cases, a formal cross-reference (the title of the page you're linking to) is the preferred style because it provides context and helps readers understand where they're going when they choose the link. Follow these guidelines for formal cross-references: - Introduce links with formal introductory text: - Use \"For information *about*\" or \"For more information *about*.\" Don't use \"For information *on*.\" - If you are linking to procedures, you can use either \"For instructions *on*\" or \"instructions *for*.\" Don't use \"instructions *about*.\" - Where space is limited (for example, in a table), you can use \"*See* [link text].\" Don't use *go to*. - Ensure that the link text matches the section title text. Example: \"To get involved, see [Contributing](https://opensearch.org/source.html) on the OpenSearch website.\" - **Embedded links**: Embedded links are woven into a sentence without formal introductory text. They're especially useful in tables or other elements where space is tight. The text around the embedded link must relate to the information in the link so that the reader understands the context. Do not use *here* or *click here* for link text because it creates accessibility problems. Example: \"Finally, [delete the index](https://opensearch.org/docs/latest/api-reference/index-apis/delete-index).\" ### Numbers and measurement - Spell out cardinal numbers from 1 to 9. For example, one NAT instance. Use numerals for cardinal numbers 10 and higher. Spell out ordinal numbers: first, second, and so on. In a series that includes numbers 10 or higher, use numerals for all. Use a comma separator for numbers of four digits or more—for example, 1,000. - For descriptions that include time ranges, separate the numbers with an en dash. Avoid extra words such as between or from n to n. - Correct: It can take 5–10 minutes before logs are available. - Incorrect: It can take between 5 and 10 minutes before logs are available. - Use numerals for all measurement-based references, including time. Include a space between the number and the abbreviation for the unit of measure. - Correct: - 100 GB - 1 TB - 3 minutes - 12 subnets (8 public and 4 private) - Incorrect - One hundred GB - 1TB ### Procedures A procedure is a series of numbered steps that a user follows to complete a specific task. Users should be able to scan for and recognize procedures easily. Make procedures recognizable by using the following: - Predictable content parts - Parallel language constructions - Consistent formatting Replace pointer-specific language with device-agnostic language to accommodate readers with disabilities and users of various input methods and devices, including the pointer, keyboard, and touch screens. For example, instead of the term *click*, which is pointer-specific, use *choose*, which is more generic and device-agnostic. However, when the generic language makes it difficult to understand the instructions (for example, in the case of opening a context menu), you can include pointer-specific hints. Use your judgment. If you have a question, ask your editor. Use the following language to describe UI interactions: - Use *choose* to describe moving to a UI component such as a tab or pane or taking action on a button or menu. - Use *select* to describe picking a user-specific resource or enabling one of several options. Contrast with *clear* to turn off previously selected options. - Use *press* to describe single key or key combination entries that users would perform on a keyboard. - Use *enter* to describe information that users add using a keyboard. - Do not use *hit* or *strike*. The following table provides examples of language to be used to describe interactions with UI elements. | UI element | Language | Example | :--------- | :------- | :------ | Menu | On the *menu* menu, choose *command*. | On the **Edit** menu, choose **Copy**. | Cascading menu | [For AWS] On the navigation bar, choose *menu*, *submenu*, …, *command*. [For conventional Windows UIs] On the menu bar, choose *menu*, *submenu*, …, *command*. | On the navigation bar, choose **AWS**, **Create a Resource Group**. | Context menu | Open the context (right-click) menu for *item*, and then choose *command*. [Cascading] Open the context (right-click) menu for *item*, and then choose *submenu*, ..., *command*. | Open the context (right-click) menu for an AMI, and then choose **Launch Instance**. Open the context (right-click) menu for the instance, and then choose **Networking**, **Manage Private IP Addresses**. | Command button | Choose *command*. | Choose **Next**. | Option button | Choose *option*. For *label*, choose *option*. | For **Type of key to generate**, choose **SSH-2 RSA**. | Check box | Select *label*. Clear *label*. | To grant read access to anonymous requests, select **Make everything public**. | List box or dropdown | For *label*, choose *item*. | For **Backup Retention Period**, choose **0**. | Text box | For *label*, enter *text*. [Combo box] For *label*, specify *xyz*. | For **Program/script**, enter `Powershell.exe`. For **Source**, specify the table name. | Toggle switch | Turn on *text*. Turn off *text*. | Turn on **Expiration date**, and then choose **Confirm**. | Other controls | Specify the type of control only if it’s helpful or unavoidable, and use the verb *choose*. | On the **Configure Security Group** page, choose an existing security group, and then choose **Next**. | Double-clicking | Replace with menu instructions, or use a generic term such as \"open.\" If double-clicking is the best or most familiar method, include it in parentheses. In general, use your best judgment depending on the context and your audience. | In AWS Explorer, open **Amazon VPC**, **VPCs**. On the **VPCs** tab, choose **Create VPC**. To display the EC2 Instances view, open the context (right-click) menu for the **Instances** node, and then choose **View**. (Or double-click the node.) | Displaying tooltips | Choose *item*. | In the **Your repositories** area, choose the target repository name to display the GitHub user or organization. | Selecting items | Select the *item*. | Select the row of the parameter group that you want to delete. | Following is an example of procedure phrasing and formatting from Amazon EC2. ![Procedure example](/images/procedures.PNG) ### Punctuation and capitalization - Use contractions carefully for a more casual tone. Use common contractions. Avoid future tense (I’ll), archaic (‘twas), colloquial (ain’t), or compound (couldn’t’ve) contractions. - Use sentence case for titles, headings, and table headers. Titles of standalone documents may use title case. - Use lowercase for nouns and noun phrases that are not proper nouns; for example, *big data*. This style follows the standard rules of American English grammar. - For plural forms of nouns that end in “s”, form the possessive case by adding only an apostrophe. - When a colon introduces a list of words, a phrase, or other sentence fragment, the first word following the colon is lowercased unless it is a proper name. When a colon introduces one or more complete sentences, the first word following it is capitalized. When text introduces a table or image, it should be a complete sentence and end with a period, not a colon. - Use commas to separate the following: - Independent clauses separated by coordinating conjunctions (but, or, yet, for, and, nor, so). - Introductory clauses, phrases, words that precede the main clause. - Words, clauses, and phrases listed in a series. Also known as the Oxford comma. - Skip the comma after single-word adverbs of time at the beginning of a sentence, such as *afterward*, *then*, *later*, or *subsequently*. - An em dash (—) is the width of an uppercase M. Do not include spacing on either side. Use an em dash to set off parenthetical phrases within a sentence or set off phrases or clauses at the end of a sentence for restatement or emphasis. - An en dash (–) is the width of an uppercase N. In ranges, do not include spacing on either side. Use an en dash to indicate ranges in values and dates, separate a bullet heading from the following text in a list, or separate an open compound adjective (two compounds, only one of which is hyphenated) from the word that it modifies. - Words with prefixes are normally closed (no hyphen), whether they are nouns, verbs, adjectives, or adverbs. Note that some industry terms don’t follow this hyphenation guidance. For example, *Command Line Interface* and *high performance computing* aren’t hyphenated, and *machine learning* isn’t hyphenated when used as an adjective. Other terms are hyphenated to improve readability. Examples include *non-production*, *post-migration*, and *pre-migration*. - The ampersand (&) should never be used in a sentence as a replacement for the word and. An exception to this is in acronyms where the ampersand is commonly used, such as in Operations & Maintenance (O&M). - When using a forward slash between words, do not insert space on either side of the slash. For example, *AI/ML* is correct whereas *AI / ML* is incorrect. - When referring to API parameters, capitalize *Boolean*. Otherwise, primitive Java data types (*byte*, *short*, *int*, *long*, *float*, *double*, and *char*) start with a lowercase letter, while non-primitive types start with an uppercase letter. ### Topic titles Here are two styles you can use for topic titles: * *Present participle phrase* + *noun-based phrase* or *present participle phrase* + *preposition* + *noun-based phrase*, used most often for concept or task topics. For example: * Configuring security * Visualizing your data * Running queries in the console * *Noun-based phrase*, used most often for reference topics. For example: * REST API reference * OpenSearch CLI * Field types * Security analytics ## UI text Consistent, succinct, and clear text is a critical component of a good UI. We help our users complete their tasks by providing simple instructions that follow a logical flow. ### UI best practices * Follow the OpenSearch Project [naming conventions, voice, tone, and brand personality traits](#naming-conventions-voice-tone-and-brand-personality-traits) guidelines. * Be consistent with other elements on the page and on the rest of the site. * Use sentence case in the UI, except for product names and other proper nouns. ### UI voice and tone Our UI text is people oriented and focused on empowering the user directly. We use language that is conversational, welcoming, engaging, and open and that emphasizes what the user can do with OpenSearch rather than what tasks OpenSearch can perform. The overall tone is knowledgeable but humble, informal but authoritative, informative but not dry, and friendly without being overly familiar. We talk to readers in their own words, never assuming that they understand how OpenSearch works. We use precise technical terms where appropriate, but we avoid technical jargon and insider lingo. We speak to readers in simple, plain, everyday language. For more information, see [Voice and tone](#voice-and-tone) and [Brand personality traits](#brand-personality-traits). ### Writing guidelines UI text is a critical component of a user interface. We help users complete tasks by explaining concepts and providing simple instructions that follow a logical flow. We strive to use language that is consistent, succinct, and clear. #### What's the purpose of UI text? UI text includes all words, phrases, and sentences on a screen, and it has the following purposes: * Describes a concept or defines a term * Explains how to complete a task * Describes the purpose of a page, section, table, graph, or dialog box * Walks users through tutorials and first-run experiences * Provides context and explanation for individual UI elements that might be unfamiliar to users * Helps users make a choice or decide if settings are relevant or required for their particular deployment scenario or environment * Explains an alert or error #### Basic guidelines Follow these basic guidelines when writing UI text. ##### Style * Keep it short. Users don’t want to read dense text. Remember that UI text can expand by 30% when it’s translated into other languages. * Keep it simple. Try to use simple sentences (one subject, one verb, one main clause and idea) rather than compound or complex sentences. * Prefer active voice over passive voice. For example, \"You can attach up to 10 policies\" is active voice, and \"Up to 10 policies can be attached\" is passive voice. * Use device-agnostic language rather than mouse-specific language. For example, use _choose_ instead of _click_ (exception: use _select_ for check boxes). ##### Tone * Use a tone that is knowledgeable but humble, informal but authoritative, informative but not dry, and friendly without being overly familiar. * Use everyday language that most users will understand. * Use second person (you, your) when you address the user. * Use _we_ if you need to refer to the OpenSearch Project as an organization; for example, \"We recommend….\" ##### Mechanics * Use sentence case for all UI text. (Capitalize only the first word in a sentence or phrase as well as any proper nouns, such as service names. All other words are lowercase.) * Use parallel construction (use phrases and sentences that are grammatically similar). For example, items in a list should start with either all verbs or all nouns. **Correct** Snapshots have two main uses: * Recovering from failure * Migrating from one cluster to another **Incorrect** Snapshots have two main uses: * Failure recovery * Migrating from one cluster to another * Use the serial (Oxford) comma. For example, “issues, bug fixes, and features”, not “issues, bug fixes and features”. * Don’t use the ampersand (&). * Avoid Latinisms, such as _e.g._, _i.e._, or _etc._ Instead of _e.g._, use _for example_ or _such as_. Instead of _i.e._, use _that is_ or _specifically_. Generally speaking, _etc._ and its equivalents (such as _and more_ or _and so on_) aren’t necessary. ## Special considerations for blog posts Blog posts provide an informal approach to educating or inspiring readers through the personal perspective of the authors. Brief posts generally accompany service or feature releases, and longer posts may note best practices or provide creative solutions. Each post must provide a clear community benefit. To enhance the strengths of the blogging platform, follow these post guidelines: **Be conversational and informal.** Posts tend to be more personable, unlike technical documentation. Ask questions, include relevant anecdotes, add recommendations, and generally try to make the post as approachable as possible. However, be careful of slang, jargon, and phrases that a global audience might not understand. **Keep it short.** Deep topics don’t necessarily require long posts. Shorter, more focused posts are easier for readers to digest. Consider breaking a long post into a series, which can also encourage repeat visitors to the blog channel. **Avoid redundancy.** Posts should add to the conversation. Instead of repeating content that is already available elsewhere, link to detail pages and technical documentation. Keep only the information that is specific to the post solution or recommendations. **Connect with other content.** All posts should contain one or more calls to action that give readers the opportunity to create resources, learn more about services or features, or connect with other community members. Posts should also include metadata tags such as services, solutions, or learning levels to help readers navigate to related content. ## Inclusive content When developing OpenSearch Project documentation, we strive to create content that is inclusive and free of bias. We use inclusive language to connect with the diverse and global OpenSearch Project audience, and we are careful in our word choices. Inclusive and bias-free content improves clarity and accessibility of our content for all audiences, so we avoid ableist and sexist language and language that perpetuates racist structures or stereotypes. In practical terms, this means that we do not allow certain terms to appear in our content, and we avoid using others, *depending on the context*. Our philosophy is that we positively impact users and our industry as we proactively reduce our use of terms that are problematic in some contexts. Instead, we use more technically precise language and terms that are inclusive of all audiences. ### Offensive terms The following terms may be associated with unconscious racial bias, violence, or politically sensitive topics and should not appear in OpenSearch Project content, if possible. Note that many of these terms are still present but on a path to not being supported. For example, `slave` was removed from the Python programming language in 2018, and the open-source community continues to work toward replacing these terms. | Don’t use | Guidance/Use instead |----------------|-----------------------------| abort | Don't use because it has unpleasant associations and is unnecessarily harsh sounding. Use *stop*, *end*, or *cancel* instead. | black day | blocked day | blacklist | deny list | execute | Replace with a more specific verb. In the sense of carrying out an action, use *run*, *process*, or *apply*. In the sense of initiating an operation, use *start*, *launch*, or *initiate*. Exception: *Execution* is unavoidable for third-party terms for which no alternative was determined, such as SQL execution plans. *Executable* is also unavoidable. | hang | Don't use. This term is unnecessarily violent for technical documentation. Use *stop responding* instead. | kill | Don't use. Replace with *stop*, *end*, *clear*, *remove*, or *cancel*. Exception: *Kill* is unavoidable when referring to Linux kill commands. | master | primary, main, leader | master account | management account | slave | replica, secondary, standby | white day | open day | whitelist | allow list | ### Sensitive terms The following terms may be problematic *in some contexts*. This doesn’t mean that you can’t use these terms—just be mindful of their potential associations when using them, and avoid using them to refer to people. | Avoid using | Guidance/Use instead |--------------------------|-------------------------------------| blackout | service outage, blocked | demilitarized zone (DMZ) | perimeter network, perimeter zone | disable | Use *turn off*, *deactivate*, or *stop* instead of *disable* to support bias-free documentation, when possible. Otherwise, use *disable* to describe making a feature or command unavailable. If the UI uses *active* and *inactive* to describe these states, use *activate* and *deactivate* in the documentation. Don't use *disable* to refer to users. | enable | Use *turn on*, *activate*, or *start* instead of *enable* to support bias-free documentation, when possible. Otherwise, use *enable* to describe making a feature or command available. If the UI uses *active* and *inactive* to describe these states, use *activate* and *deactivate* in the documentation. Avoid using *enable* to refer to making something possible for the user. Instead, rewrite to focus on what's important from the user's point of view. For example, “With ABC, you can do XYZ” is a stronger statement than “ABC enables you to XYZ.” Additionally, using a task-based statement is usually more clear than the vague “…enables you to….” | invalid | not valid | primitive | Avoid using *primitive* (especially plural *primitives*) as a colloquial way of referring to the basic concepts or elements that are associated with a feature or to the simplest elements in a programming language. For greatest clarity and to avoid sounding unpleasant, replace with *primitive data type* or *primitive type*. | purge | Use only in reference to specific programming methods. Otherwise, use *delete*, *clear*, or *remove* instead. | segregate | separate, isolate | trigger | Avoid using as a verb to refer to an action that precipitates a subsequent action. It is OK to use when referring to a feature name, such as a *trigger function* or *time-triggered architecture*. As a verb, use an alternative, such as *initiate*, *invoke*, *launch*, or *start*. | ## Trademark policy The “OpenSearch” word mark should be used in its exact form and not abbreviated or combined with any other word or words (e.g., “OpenSearch” software rather than “OPNSRCH” or “OpenSearch-ified”). See the [OpenSearch Trademark Policy](https://opensearch.org/trademark-usage.html) for more information. Also refer to the policy and to the [OpenSearch Brand Guidelines](https://opensearch.org/brand.html) for guidance regarding the use of the OpenSearch logo. When using another party’s logo, refer to that party’s trademark guidelines. ",
    "url": "http://localhost:4000/STYLE_GUIDE/",
    "relUrl": "/STYLE_GUIDE/"
  },"24": {
    "doc": "OpenSearch terms",
    "title": "OpenSearch terms",
    "content": "# OpenSearch terms This is how we use our terms, but we’re always open to hearing your suggestions. ## A **abort** Do not use because it has unpleasant associations and is unnecessarily harsh sounding. Use *stop*, *end*, or *cancel* instead. **above** Use only for physical space or screen descriptions, for example, \"the outlet above the floor\" or \"the button above the bar pane.\" For orientation within a document use *previous*, *preceding*, or *earlier*. **ad hoc** Avoid. Use *one-time* instead. **affect** Affect as a noun refers to emotion as expressed in face or body language. Affect as a verb means to influence. Do not confuse with effect. **AI/ML** On first mention, use artificial intelligence and machine learning (AI/ML). **Alerting** A plugin that notifies you when data from one or more OpenSearch indexes meets certain conditions. **allow** Use allow when the user must have security permissions in order to complete the task. Avoid using allow to refer to making something possible for the user. Instead, rewrite to focus on what’s important from the user’s point of view. **allow list** Use to describe a list of items that are allowed (not blocked). Do not use as a verb. Do not use whitelist. **Amazon OpenSearch Service** Amazon OpenSearch Service is a managed service that makes it easy to deploy, operate, and scale OpenSearch clusters in the AWS Cloud. Amazon OpenSearch Service is the successor to Amazon Elasticsearch Service (Amazon ES) and supports OpenSearch and legacy Elasticsearch OSS (up to 7.10, the final open-source version of the software). **Anomaly Detection** A plugin that automatically detects anomalies in your OpenSearch data in near real time. **API operation** Use instead of action, method, or function. OpenSearch style: - Use the CopySnapshot operation to... - The following API operations… Not OpenSearch style - Use the CopySnapshot action to... - Use the CopySnapshot method to... - Use the CopySnapshot function to... **app or application** Use app for mobile software, application for all other uses. **appear, display, and open** Messages and pop-up boxes appear. Windows, pages, and applications open. The verb display requires a definite object. For example: The system displays the error message. **application server** Do not abbreviate as app server. **as well as** Avoid. Replace with in addition to or and as appropriate. **Asynchronous Search** A plugin that lets the user send search requests in the background so that the results can be used later. **auto scaling** Lower case scaling, auto scaling, and automatic scaling (but not autoscaling) are the preferred descriptive terms when generically describing auto scaling functionality. Do not use hyphenated auto-scaling as a compound modifier. Instead, use scaling (for example, scaling policy), or scalable (for example, scalable target or scalable, load-balanced environment). ## B **below** Use only for physical space or screen descriptions, such as “the outlet below the vent,” or “the button below the bar pane.” For orientation within a document, use *following* or *later*. **big data** **black day** Do not use. Use *blocked day* instead. **blacklist** Do not use. Use *deny list* instead. **blackout** Avoid using. Use *service outage* or *blocked* instead. **BM25** A ranking function used to estimate the relevance of documents to a given search query. BM25 extends [TF–IDF](#t) by normalizing document length. **Boolean** Avoid using the name of a Boolean value at the beginning of a sentence or sentence fragment. In general, capitalize the word Boolean. For specific programming languages, follow the usage in that language. OpenSearch style: - You can use the Boolean functions with Boolean expressions or integer expressions. - IsTruncated(): A Boolean value that specifies whether the resolved target list is truncated. **bottom** Use only as a general screen reference, such as “scroll to the bottom of the page.” Don’t use for window, page, or pane references to features or controls. Rather, use *lower* instead. For example, you can use the following wording: “Choose the button on the lower left.” **browse** Use when referring to scanning information or browsing the web. Don’t use when describing how to navigate to a particular item on our site or a computer. Instead, use *see* or *navigate to*. **build (n., v.)** Use as a verb to refer to compiling and linking code. Use as a noun only to refer to a compiled version of a program (for example, *Use the current build of Amazon Linux 2*...) in a programming reference. ## C **CA** certificate authority **certs, certificates** Use _certificates_ on first mention. It’s OK to use _certs_ thereafter. **CI/CD** Use _continuous integration_ and _continuous delivery (CI/CD)_ or _continuous integration and delivery (CI/CD)_ on first mention. **cluster** A collection of one or more nodes. **cluster manager** A single node that routes requests for the cluster and makes changes to other nodes. Each cluster contains a single cluster manager. **console** A tool inside OpenSearch Dashboards used to interact with the OpenSearch REST API. **Cross-Cluster Replication** A plugin that replicates indexes, mappings, and metadata from one OpenSearch cluster to another. Follows an active-passive model where the follower index pulls data from a leader index. **cyber** Except when dictated by open standards, use as a prefix in a closed compound: don’t use spaces or hyphens between _cyber_ and the rest of the word. ## D **data** Use data is, not data are. Don’t use datas. Use pieces of data or equivalent to describe individual items within a set of data. **data center** **dataset** **data store, datastore** Two words when used generically, but one word when referring to the VMware product. **data type** **dates** Use one of the following date formats: - When a human-readable date format is preferred, spell out the date using the Month D, YYYY format (for example, _October 1, 2022_). Do not use an ordinal number for the day (use _1_, not _1st_). If the context is clear, you can omit the year on subsequent mention. If the specific day isn’t known, use the Month YYYY format (for example, _October 2022_). - When a numeric, lexicographically sortable date is required, use the YYYY-MM-DD format (for example, _2022-10-01_). Make sure to add a zero (0) in front of a single-digit month and day. This is the ISO 8601 standard date format. Make sure also that you use a hyphen (-) and avoid omitting the year. Doing so avoids the ambiguity that’s caused by the common, locally used formats of MM/DD and DD/MM. **demilitarized zone (DMZ)** Avoid using. Use *perimeter network* or *perimeter zone* instead. **deny list** Use to describe a list of items that aren’t allowed (blocked). Do not use _blacklist_. **disable** Use *turn off*, *deactivate*, or *stop* instead of *disable* to support bias-free documentation, when possible. Otherwise, use *disable* to describe making a feature or command unavailable. If the UI uses *active* and *inactive* to describe these states, use *activate* and *deactivate* in the documentation. Don't use *disable* to refer to users. **double-click** Always hyphenated. Don’t use _double click_. **dropdown list** **due to** Don’t use. Use _because of_ instead. ## E **effect** _Effect_ as a noun refers to something that’s caused by something else. _Effect_ as a verb means to bring about. Do not confuse with _affect_. **e.g.** Avoid. Use for example or such as instead. **Elastic IP address** **email** Use as a singular noun or adjective to refer to the collective concept, and use _message_ or _mail_ for individual items. Use _send email_ as the verb form. Don’t use the plural form because it’s a collective noun. **enable** Use _turn on_ or _activate_ instead of *enable* to support bias-free documentation, when possible. Otherwise, use *enable* to describe making a feature or command available. If the UI uses *active* and *inactive* to describe these states, use *activate* and *deactivate* in the documentation. Avoid using *enable* to refer to making something possible for the user. Instead, rewrite to focus on what's important from the user's point of view. For example, “With ABC, you can do XYZ” is a stronger statement than “ABC enables you to XYZ.” Additionally, using a task-based statement is usually more clear than the vague “…enables you to….” **enter** In general, use in preference to _type_ when a user adds text or other input (such as numbers or symbols). **etc., et cetera** Do not use. Generally speaking, etc. and its equivalents (such as and more or and so on) aren’t necessary. **execute** Replace with a more specific verb. In the sense of carrying out an action, use *run*, *process*, or *apply*. In the sense of initiating an operation, use *start*, *launch*, or *initiate*. Exception: *Execution* is unavoidable for third-party terms for which no alternative was determined, such as SQL execution plans. *Executable* is also unavoidable. ## F **fail over (v.), failover (n.)** **file name** **frontend (n., adj.)** Use frontend as an adjective and a noun. Do not use front end or front-end. Do not make frontend possessive except as part of a compound noun, such as frontend system. ## G **geopoint** **geoshape** ## H **hang** Do not use. This term is unnecessarily violent for technical documentation. Use *stop responding* instead. **hardcode** **hard disk drive (HDD)** **high availability (HA)** **high performance computing (HPC)** **hostname** ## I **i.e.** Do not use. Use _that_ is or _specifically_ instead. **if, whether** Do not use *if* to mean *whether*. It is best to use *whether* in reference to a choice or alternatives (\"we're going whether it rains or not\") and *if* when establishing a condition (\"we will go if it doesn't rain\"). **in, on** Use _in Windows_ or _in Linux_ in reference to components of the OS or work in the OS. Use on Windows in reference to Windows applications. Examples: - Use the Devices and Printers Control Panel in Windows to install a new printer. - In Windows, run the setup command. - Select an application that runs on Windows. Run applications and instances _in the cloud_, but extend services to the cloud. Use *on the forum*. Whatever is on the internet (the various websites, etc.), you are *on* because you cannot be *in* it. **index, indexes** A collection of JSON documents. Non-hardcoded references to *indices* should be changed to *indexes*. **Index Management (IM)** **Index State Management (ISM)** **install in, on** install in a folder, directory, or path; install on a disk, drive, or instance. **internet** Do not capitalize. **invalid** Avoid using. Use *not valid* instead. **IP address** Don’t abbreviate as _IP only_. ## K **kill** Do not use. Replace with *stop*, *end*, *clear*, *remove*, or *cancel*. Exception: *Kill* is unavoidable when referring to Linux kill commands. **k-means** A simple and popular unsupervised clustering ML algorithm built on top of Tribuo library that chooses random centroids and calculates iteratively to optimize the position of the centroids until each observation belongs to the cluster with the nearest mean. **k-NN** Short for _k-nearest neighbors_, the k-NN plugin enables users to search for the k-nearest neighbors to a query point across an index of vectors. ## L **launch, start** You _start_ an application but _launch_ an instance, environment, or cluster. **let** Avoid using _let_ to refer to making something in a service or feature possible for the user. Instead, rewrite to focus on what’s important from the user’s point of view. **leverage** Replace with _use_. **lifecycle** One word in reference to software. **like (prep.)** OK to use to call out something for comparison. As a general rule, if you can replace like with similar to, it’s OK to use like. But, if you can replace _like_ with _such as_, use _such as_. **locate in, on** Located _in_ (a folder, directory, path), located on a disk drive or instance. **log in (v.), login (adj., n.)** Use with technologies with interfaces that use this verb. Also note that you log in to an instance, not log into. Also use log out and logout. **LogStash** A light-weight, open-source, server-side data processing pipeline that allows you to collect data from a variety of sources, transform it on the fly, and send it to your desired destination. **lower left, lower right** Hyphenate as adjectives. Use instead of *bottom left* and *bottom right*, unless the field name uses *bottom*. For example, \"The lower-right corner.\" **LTS** Long-Term Support **Lucene** Apache Lucene™ is a high-performance, full-featured search engine library written entirely in Java. OpenSearch uses a modified version of Lucene as the basis for search operations within OpenSearch. ## M **machine learning** Write as two words (no hyphen) in all cases, including when used as an adjective before a noun. Abbreviate to ML after first use if appropriate. **Machine Learning (ML) Commons** A new plugin that makes it easy to develop new ML features. It allows engineers to leverage existing open-source ML algorithms and reduce the efforts to build them from scratch. **master** Do not use. Use *primary*, *main*, or *leader* instead. **master account** Do not use. Use *management account* instead. **may** Avoid. Use _can_ or _might_ instead. **must, shall, should** _Must_ and _shall_ refer to requirements. If the reader doesn’t follow the instruction, something won’t work right. _Should_ is used with recommendations. If the reader doesn’t follow the instruction, it might be harder or slower, but it’ll work. ## N **navigate to** Not navigate _in_. **near real time (n.), near real-time (adj.) (NRT)** Use _near real time_ as a noun; use near real-time as an adjective. Don’t add a hyphen between _near_ and _real time_ or _real-time_. Spell out _near real time_ on first mention; _NRT_ can be used on subsequent mentions. **node** A server that stores your data and processes search requests with OpenSearch, usually as part of a cluster. Do not use _master node_ and avoid using _worker node_. **non-production** Hyphenate to make the term easier to scan and read. ## O **onsite** **OpenSearch** OpenSearch is a community-driven, open-source search and analytics suite derived from Apache 2.0 licensed Elasticsearch 7.10.2 and Kibana 7.10.2. It consists of a search engine daemon, OpenSearch, and a visualization and user interface, OpenSearch Dashboards. **OpenSearch Dashboards** The default visualization tool for data in OpenSearch. On first appearance, use the full name. “Dashboards” may be used on subsequent appearances. open source (n.), open-source (adj.) Use _open source_ as a noun (for example, “The code used throughout this tutorial is open source and can be freely modified”). Use _open-source_ as an adjective _(open-source software)_. **operating system** When referencing operating systems in documentation, follow these guidelines: - In general, if your docs or procedures apply to both Linux and macOS, you can also include Unix. - Unix and UNIX aren’t the same. UNIX is a trademarked name that’s owned by The Open Group. In most cases, you should use Unix. - When referring to the Mac operating system, use macOS. Don’t say Mac, Mac OS, or OS X. - When referring to Windows, it’s not necessary to prefix with Microsoft. - If you need to reference multiple Unix-like operating systems, you should separate by commas and use the following order: Linux, macOS, or Unix. **or earlier, or later** OK to use with software versions. ## P **Painless** The default scripting language for OpenSearch, either used inline or stored for repeat use. Similar to Java’s language specification. **per** - Do not use to mean _according to_ (for example, per the agreement). - OK to use in meaning of _to_, _in_, _for_, or _by each_ (one per account) where space is limited and in set terms and phrases, such as any of the following: - queries per second (QPS) - bits per second (bps) - megabytes per second (MBps) - Consider writing around _per_ elsewhere. _Per_ can sound stuffy and confusing to some global users. **percent** Spell out in blog posts (for example, 30 percent). Use % in headlines, quotations, and tables or in technical copy. **Performance Analyzer** An agent and REST API that allows you to query numerous performance metrics for your cluster, including aggregations of those metrics, independent of the Java Virtual Machine (JVM). **please** Avoid using except in quoted text. **plugin** Tools inside of OpenSearch that can be customized to enhance OpenSearch’s functionality. For a list of core plugins, see the [OpenSearch plugin installation]({{site.url}}{{site.baseurl}}/opensearch/install/plugins/) page. Capitalize if it appears as part of the product name in the UI. **pop-up** **precision** The accuracy of the results returned from a query. **premise, premises** With reference to property and buildings, always form as plural. Correct: an on-premises solution Incorrect: an on-premise solution, an on-prem solution **primary shard** A Lucene instance that contains data for some or all of an index. **primitive** Avoid using *primitive* (especially plural *primitives*) as a colloquial way of referring to the basic concepts or elements that are associated with a feature or to the simplest elements in a programming language. For greatest clarity and to avoid sounding unpleasant, replace with *primitive data type* or *primitive type*. **purge** Use only in reference to specific programming methods. Otherwise, use *delete*, *clear*, or *remove* instead. ## Q **query** A call used to request information about your data. ## R **real time (n.) real-time (adj.)** Use with caution; this term can imply a degree of responsiveness or speed that may not be true. When needed, use _real time_ as a noun (for example “The request is sent in real time”). Use _real-time_ as an adjective (“A real-time feed is displayed...”). **recall** The quantity of documents returned from a query. **replica shard** Copy of a primary shard. Helps improve performance when using indexes across multiple nodes. **repo** Use as a synonym for repository, on second and subsequent use. **RPM Package Manager (RPM)** Formerly known as RedHat Package Manager. An open-source package management system for use with Linux distributions. **rule** A set of conditions, internals, and actions that create notifications. ## S **screenshot** **segregate** Avoid using. Use *separate* or *isolate* instead. **setting** A key-value pair that creates a mapping in one of the many YAML configuration files used throughout OpenSearch. Sometimes alternatively called parameters, the programming language manipulating the key-value pair usually dictates the name of this mapping in a YAML file. For OpenSearch documentation (Java), they are properly a `Setting` object. The following examples of settings illustrate key-value pairs with a colon separating the two elements: `Settings.index.number_of_shards: 4` `plugins.security.audit.enable_rest: true` **set up (v.), setup (n., adj.)** Use _set up_ as a verb (“To set up a new user...”). Use _setup_ as a noun or adjective (“To begin setup...”). **shard** A piece of an index that consumes CPU and memory. Operates as a full Lucene index. **since** Use only to describe time events. Don’t use in place of because. **slave** Do not use. Use *replica*, *secondary*, or *standby* instead. **Snapshot Management (SM)** **solid state drive (SSD)** **standalone** **start, launch** You _start_ an application but _launch_ an instance, environment, or cluster. **startup (n.), start up (v.)** Never hyphenated. Use _startup_ as a noun (for example, “The following startup procedure guides you through...”). Use _start up_ as a verb (“You can start up the instances by...”). **Stochastic Gradient Descent (SGD)** ## T **term frequency–inverse document frequency (TF–IDF)** A numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. **time out (verb), timeout (noun, adjective)** Never hyphenate. Use _time out_ as a verb (“The request will time out if the server doesn’t respond”). Use _timeout_ as a noun or adjective (“You can set the timeout interval by entering a number into...”). **time frame** **time-series data** Data that's provided as part of a metric. The time value is assumed to be when the value occurred. **timestamp** **time zone** **trigger** Avoid using as a verb to refer to an action that precipitates a subsequent action. It is OK to use when referring to a feature name, such as a *trigger function* or *time-triggered architecture*. As a verb, use an alternative, such as *initiate*, *invoke*, *launch*, or *start*. ## U **UltraWarm** A storage tier that you can use to store and analyze your data with Elasticsearch and Kibana that is optimized for performance. To learn more about the service, see the introductory [blog post](https://aws.amazon.com/about-aws/whats-new/2020/05/aws-announces-amazon-elasticsearch-service-ultrawarm-general-availability/). **upper left, upper right** Hyphenate as adjectives. Use instead of *top left* and *top right*, unless the field name uses *top*. For example, \"The upper-right corner.\" **US** No periods, as specified in the Chicago Manual of Style. **user** In most cases, replace with the more direct form you. Reserve _user_ for cases where you are referring to a third party (not the audience you are writing for). **username** ## V **version** **v., vs., versus** Do not use. Use _compared_ to or _compared with_ instead. **via** Do not use. Replace with by using, through, or with or a more specific phrase such as by accessing or by choosing. ## W **web** **webpage** Never _web page_. **website** Never _web site_. **while, although, whereas** Only use _while_ to mean “during an interval of time.” Don’t use it to mean although because it is often ambiguous. _Whereas_ is a better alternative to although in many cases, but it can sound overly formal. **white day** Do not use. Use *open day* instead. **whitelist** Do not use. Use *allow list* instead. **wish, want, desire, need** _Wish_ and _desire_ are indirect and nuanced versions of _want_. Don’t use them. Be direct. Do not confuse wants with needs. Use the term that’s appropriate to the situation. _Need_ connotes a requirement or obligation, whereas _want_ indicates that you have an intent but still a choice of valid actions. ## Y **Yellowdog Updater Modified (YUM)** An open-source tool for command-line and graphical-based package management for RPM (RedHat Package Manager)-based Linux systems. ",
    "url": "http://localhost:4000/TERMS/",
    "relUrl": "/TERMS/"
  },"25": {
    "doc": "OpenSearch Documentation Website 2.4.0 Release Notes",
    "title": "OpenSearch Documentation Website 2.4.0 Release Notes",
    "content": "# OpenSearch Documentation Website 2.4.0 Release Notes The following page provides a list of updates made to OpenSearch documentation for OpenSearch 2.4. - Add Alerting RBAC backend roles [#1866](https://github.com/opensearch-project/documentation-website/pull/1866) - Adds Windows Installation documentation [#1825](https://github.com/opensearch-project/documentation-website/pull/1825) - Adds Snapshot restore documentation [#1822](https://github.com/opensearch-project/documentation-website/pull/1822) - Adds Search relevance documentation [#1801](https://github.com/opensearch-project/documentation-website/pull/1801) - Add documentation for Aggregate view of saved objects by tenant in Dashboards [#1786](https://github.com/opensearch-project/documentation-website/pull/1786) - Adds search backpressure documentation [#1790](https://github.com/opensearch-project/documentation-website/pull/1790) - Adds Windows security documentation [#1821](https://github.com/opensearch-project/documentation-website/pull/1821) - Adds xy point and shape documentation [#1564](https://github.com/opensearch-project/documentation-website/pull/1564) - Adds Point in Time documentation [#1753](https://github.com/opensearch-project/documentation-website/pull/1753) - Adds GeoHex grid documentation [#1761](https://github.com/opensearch-project/documentation-website/pull/1761) - Adds GeoJSON format to geopoint [#1806](https://github.com/opensearch-project/documentation-website/pull/1806) - Add connection pooling settings to LDAP documentation [#1698](https://github.com/opensearch-project/documentation-website/pull/1698) - Add search with k-NN filters [#1814](https://github.com/opensearch-project/documentation-website/pull/1814) - Update snapshot documentation with storage_type option [#1749](https://github.com/opensearch-project/documentation-website/pull/1749) - Add searchable snapshots [#1795](https://github.com/opensearch-project/documentation-website/pull/1795) - Add configuring Dashboards multi-authentication sign-in window [#1549](https://github.com/opensearch-project/documentation-website/pull/1549) - Add Neural Search plugin [#1882](https://github.com/opensearch-project/documentation-website/pull/1882) - Add Security Analytics plugin [#1824](https://github.com/opensearch-project/documentation-website/pull/1824) - Add Error prevention validation ISM API [#1884](https://github.com/opensearch-project/documentation-website/pull/1884) - Add model serving framework [#1880](https://github.com/opensearch-project/documentation-website/pull/1880) - Add multiple data sources in Dashboards [#1711](https://github.com/opensearch-project/documentation-website/pull/1711) ",
    "url": "http://localhost:4000/release-notes/opensearch-documentation-release-notes-2.4.0/",
    "relUrl": "/release-notes/opensearch-documentation-release-notes-2.4.0/"
  },"26": {
    "doc": "OpenSearch Documentation Website 2.5.0 Release Notes",
    "title": "OpenSearch Documentation Website 2.5.0 Release Notes",
    "content": "# OpenSearch Documentation Website 2.5.0 Release Notes The OpenSearch 2.5.0 documentation includes the following additions and updates. ## New documentation for 2.5.0 - Add k-NN filter to neural search [#2466](https://github.com/opensearch-project/documentation-website/pull/2466) - Add two new settings for Anomaly Detection 2.5 [#2450](https://github.com/opensearch-project/documentation-website/pull/2450) - Add new ML cluster settings for 2.5 [#2442](https://github.com/opensearch-project/documentation-website/pull/2442) - Adds installation guide for OpenSearch Dashboards debian distribution [#2440](https://github.com/opensearch-project/documentation-website/pull/2440) - Add cluster awareness and decommission docs [#2438](https://github.com/opensearch-project/documentation-website/pull/2438) - Add Query String for rollups [#2428](https://github.com/opensearch-project/documentation-website/pull/2428) - Update field mapping documentation for Security Analytics [#2422](https://github.com/opensearch-project/documentation-website/pull/2422) - Updates to Security Analytics documentation [#2408](https://github.com/opensearch-project/documentation-website/pull/2408) - Add new Rules documentation that covers YAML Editor view [#2407](https://github.com/opensearch-project/documentation-website/pull/2407) - Adds documentation for Admin UI index operations [#2403](https://github.com/opensearch-project/documentation-website/pull/2403) - Remove experimental warning note for Security Analytics 2.5 [#2401](https://github.com/opensearch-project/documentation-website/pull/2401) - Adds cluster health by awareness attribute documentation [#2398](https://github.com/opensearch-project/documentation-website/pull/2398) - Adds maps documentation [#2376](https://github.com/opensearch-project/documentation-website/pull/2376) - Adds Jaeger trace data for analytics documentation [#2374](https://github.com/opensearch-project/documentation-website/pull/2374) - Adds cluster manager task throttling documentation [#1826](https://github.com/opensearch-project/documentation-website/pull/1826) ## Documentation for 2.5.0 experimental features - Add GPU acceleration documentation [#2384](https://github.com/opensearch-project/documentation-website/pull/2384) - Updates remote-backed storage documentation [#2363](https://github.com/opensearch-project/documentation-website/pull/2363) ",
    "url": "http://localhost:4000/release-notes/opensearch-documentation-release-notes-2.5.0/",
    "relUrl": "/release-notes/opensearch-documentation-release-notes-2.5.0/"
  },"27": {
    "doc": "Index rollups",
    "title": "Index rollups",
    "content": "Time series data increases storage costs, strains cluster health, and slows down aggregations over time. Index rollup lets you periodically reduce data granularity by rolling up old data into summarized indices. You pick the fields that interest you and use index rollup to create a new index with only those fields aggregated into coarser time buckets. You can store months or years of historical data at a fraction of the cost with the same query performance. For example, say you collect CPU consumption data every five seconds and store it on a hot node. Instead of moving older data to a read-only warm node, you can roll up or compress this data with only the average CPU consumption per day or with a 10% decrease in its interval every week. You can use index rollup in three ways: . | Use the index rollup API for an on-demand index rollup job that operates on an index that’s not being actively ingested such as a rolled-over index. For example, you can perform an index rollup operation to reduce data collected at a five minute interval to a weekly average for trend analysis. | Use the OpenSearch Dashboards UI to create an index rollup job that runs on a defined schedule. You can also set it up to roll up your indices as it’s being actively ingested. For example, you can continuously roll up Logstash indices from a five second interval to a one hour interval. | Specify the index rollup job as an ISM action for complete index management. This allows you to roll up an index after a certain event such as a rollover, index age reaching a certain point, index becoming read-only, and so on. You can also have rollover and index rollup jobs running in sequence, where the rollover first moves the current index to a warm node and then the index rollup job creates a new index with the minimized data on the hot node. | . ",
    "url": "http://localhost:4000/im-plugin/index-rollups/index/",
    "relUrl": "/im-plugin/index-rollups/index/"
  },"28": {
    "doc": "Index rollups",
    "title": "Create an Index Rollup Job",
    "content": "To get started, choose Index Management in OpenSearch Dashboards. Select Rollup Jobs and choose Create rollup job. Step 1: Set up indices . | In the Job name and description section, specify a unique name and an optional description for the index rollup job. | In the Indices section, select the source and target index. The source index is the one that you want to roll up. The source index remains as is, the index rollup job creates a new index referred to as a target index. The target index is where the index rollup results are saved. For target index, you can either type in a name for a new index or you select an existing index. | Choose Next | . After you create an index rollup job, you can’t change your index selections. Step 2: Define aggregations and metrics . Select the attributes with the aggregations (terms and histograms) and metrics (avg, sum, max, min, and value count) that you want to roll up. Make sure you don’t add a lot of highly granular attributes, because you won’t save much space. For example, consider a dataset of cities and demographics within those cities. You can aggregate based on cities and specify demographics within a city as metrics. The order in which you select attributes is critical. A city followed by a demographic is different from a demographic followed by a city. | In the Time aggregation section, select a timestamp field. Choose between a Fixed or Calendar interval type and specify the interval and timezone. The index rollup job uses this information to create a date histogram for the timestamp field. | (Optional) Add additional aggregations for each field. You can choose terms aggregation for all field types and histogram aggregation only for numeric fields. | (Optional) Add additional metrics for each field. You can choose between All, Min, Max, Sum, Avg, or Value Count. | Choose Next. | . Step 3: Specify schedule . Specify a schedule to roll up your indices as it’s being ingested. The index rollup job is enabled by default. | Specify if the data is continuous or not. | For roll up execution frequency, select Define by fixed interval and specify the Rollup interval and the time unit or Define by cron expression and add in a cron expression to select the interval. To learn how to define a cron expression, see Alerting. | Specify the number of pages per execution process. A larger number means faster execution and more cost for memory. | (Optional) Add a delay to the roll up executions. This is the amount of time the job waits for data ingestion to accommodate any processing time. For example, if you set this value to 10 minutes, an index rollup that executes at 2 PM to roll up 1 PM to 2 PM of data starts at 2:10 PM. | Choose Next. | . Step 4: Review and create . Review your configuration and select Create. Step 5: Search the target index . You can use the standard _search API to search the target index. Make sure that the query matches the constraints of the target index. For example, if you don’t set up terms aggregations on a field, you don’t receive results for terms aggregations. If you don’t set up the maximum aggregations, you don’t receive results for maximum aggregations. You can’t access the internal structure of the data in the target index because the plugin automatically rewrites the query in the background to suit the target index. This is to make sure you can use the same query for the source and target index. To query the target index, set size to 0: . GET target_index/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggs\": { \"avg_cpu\": { \"avg\": { \"field\": \"cpu_usage\" } } } } . Consider a scenario where you collect rolled up data from 1 PM to 9 PM in hourly intervals and live data from 7 PM to 11 PM in minutely intervals. If you execute an aggregation over these in the same query, for 7 PM to 9 PM, you see an overlap of both rolled up data and live data because they get counted twice in the aggregations. ",
    "url": "http://localhost:4000/im-plugin/index-rollups/index/#create-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/index/#create-an-index-rollup-job"
  },"29": {
    "doc": "Index rollups",
    "title": "Sample Walkthrough",
    "content": "This walkthrough uses the OpenSearch Dashboards sample e-commerce data. To add that sample data, log in to OpenSearch Dashboards, choose Home and Try our sample data. For Sample eCommerce orders, choose Add data. Then run a search: . GET opensearch_dashboards_sample_data_ecommerce/_search . Sample response . { \"took\": 23, \"timed_out\": false, \"_shards\": { \"total\": 1, \"successful\": 1, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": { \"value\": 4675, \"relation\": \"eq\" }, \"max_score\": 1, \"hits\": [ { \"_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"_type\": \"_doc\", \"_id\": \"jlMlwXcBQVLeQPrkC_kQ\", \"_score\": 1, \"_source\": { \"category\": [ \"Women's Clothing\", \"Women's Accessories\" ], \"currency\": \"EUR\", \"customer_first_name\": \"Selena\", \"customer_full_name\": \"Selena Mullins\", \"customer_gender\": \"FEMALE\", \"customer_id\": 42, \"customer_last_name\": \"Mullins\", \"customer_phone\": \"\", \"day_of_week\": \"Saturday\", \"day_of_week_i\": 5, \"email\": \"selena@mullins-family.zzz\", \"manufacturer\": [ \"Tigress Enterprises\" ], \"order_date\": \"2021-02-27T03:56:10+00:00\", \"order_id\": 581553, \"products\": [ { \"base_price\": 24.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 19240, \"category\": \"Women's Clothing\", \"sku\": \"ZO0064500645\", \"taxless_price\": 24.99, \"unit_discount_amount\": 0, \"min_price\": 12.99, \"_id\": \"sold_product_581553_19240\", \"discount_amount\": 0, \"created_on\": \"2016-12-24T03:56:10+00:00\", \"product_name\": \"Blouse - port royal\", \"price\": 24.99, \"taxful_price\": 24.99, \"base_unit_price\": 24.99 }, { \"base_price\": 10.99, \"discount_percentage\": 0, \"quantity\": 1, \"manufacturer\": \"Tigress Enterprises\", \"tax_amount\": 0, \"product_id\": 17221, \"category\": \"Women's Accessories\", \"sku\": \"ZO0085200852\", \"taxless_price\": 10.99, \"unit_discount_amount\": 0, \"min_price\": 5.06, \"_id\": \"sold_product_581553_17221\", \"discount_amount\": 0, \"created_on\": \"2016-12-24T03:56:10+00:00\", \"product_name\": \"Snood - rose\", \"price\": 10.99, \"taxful_price\": 10.99, \"base_unit_price\": 10.99 } ], \"sku\": [ \"ZO0064500645\", \"ZO0085200852\" ], \"taxful_total_price\": 35.98, \"taxless_total_price\": 35.98, \"total_quantity\": 2, \"total_unique_products\": 2, \"type\": \"order\", \"user\": \"selena\", \"geoip\": { \"country_iso_code\": \"MA\", \"location\": { \"lon\": -8, \"lat\": 31.6 }, \"region_name\": \"Marrakech-Tensift-Al Haouz\", \"continent_name\": \"Africa\", \"city_name\": \"Marrakesh\" }, \"event\": { \"dataset\": \"sample_ecommerce\" } } } ] } } ... Create an index rollup job. This example picks the order_date, customer_gender, geoip.city_name, geoip.region_name, and day_of_week fields and rolls them into an example_rollup target index: . PUT _plugins/_rollup/jobs/example { \"rollup\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"last_updated_time\": 1602100553, \"description\": \"An example policy that rolls up the sample ecommerce data\", \"source_index\": \"opensearch_dashboards_sample_data_ecommerce\", \"target_index\": \"example_rollup\", \"page_size\": 1000, \"delay\": 0, \"continuous\": false, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"order_date\", \"fixed_interval\": \"60m\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"customer_gender\" } }, { \"terms\": { \"source_field\": \"geoip.city_name\" } }, { \"terms\": { \"source_field\": \"geoip.region_name\" } }, { \"terms\": { \"source_field\": \"day_of_week\" } } ], \"metrics\": [ { \"source_field\": \"taxless_total_price\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} } ] }, { \"source_field\": \"total_quantity\", \"metrics\": [ { \"avg\": {} }, { \"max\": {} } ] } ] } } . You can query the example_rollup index for the terms aggregations on the fields set up in the rollup job. You get back the same response that you would on the original opensearch_dashboards_sample_data_ecommerce source index. POST example_rollup/_search { \"size\": 0, \"query\": { \"bool\": { \"must\": {\"term\": { \"geoip.region_name\": \"California\" } } } }, \"aggregations\": { \"daily_numbers\": { \"terms\": { \"field\": \"day_of_week\" }, \"aggs\": { \"per_city\": { \"terms\": { \"field\": \"geoip.city_name\" }, \"aggregations\": { \"average quantity\": { \"avg\": { \"field\": \"total_quantity\" } } } }, \"total_revenue\": { \"sum\": { \"field\": \"taxless_total_price\" } } } } } } . Sample Response . { \"took\" : 14, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 281, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"daily_numbers\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Friday\", \"doc_count\" : 59, \"total_revenue\" : { \"value\" : 4858.84375 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 59, \"average quantity\" : { \"value\" : 2.305084745762712 } } ] } }, { \"key\" : \"Saturday\", \"doc_count\" : 46, \"total_revenue\" : { \"value\" : 3547.203125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 46, \"average quantity\" : { \"value\" : 2.260869565217391 } } ] } }, { \"key\" : \"Tuesday\", \"doc_count\" : 45, \"total_revenue\" : { \"value\" : 3983.28125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 45, \"average quantity\" : { \"value\" : 2.2888888888888888 } } ] } }, { \"key\" : \"Sunday\", \"doc_count\" : 44, \"total_revenue\" : { \"value\" : 3308.1640625 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 44, \"average quantity\" : { \"value\" : 2.090909090909091 } } ] } }, { \"key\" : \"Thursday\", \"doc_count\" : 40, \"total_revenue\" : { \"value\" : 2876.125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 40, \"average quantity\" : { \"value\" : 2.3 } } ] } }, { \"key\" : \"Monday\", \"doc_count\" : 38, \"total_revenue\" : { \"value\" : 2673.453125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 38, \"average quantity\" : { \"value\" : 2.1578947368421053 } } ] } }, { \"key\" : \"Wednesday\", \"doc_count\" : 38, \"total_revenue\" : { \"value\" : 3202.453125 }, \"per_city\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Los Angeles\", \"doc_count\" : 38, \"average quantity\" : { \"value\" : 2.236842105263158 } } ] } } ] } } } . ",
    "url": "http://localhost:4000/im-plugin/index-rollups/index/#sample-walkthrough",
    "relUrl": "/im-plugin/index-rollups/index/#sample-walkthrough"
  },"30": {
    "doc": "Index rollups",
    "title": "The doc_count field",
    "content": "The doc_count field in bucket aggregations contains the number of documents collected in each bucket. When calculating the bucket’s doc_count, the number of documents is incremented by the number of the pre-aggregated documents in each summary document. The doc_count returned from rollup searches represents the total number of matching documents from the source index. The document count for each bucket is the same whether you search the source index or the rollup target index. ",
    "url": "http://localhost:4000/im-plugin/index-rollups/index/#the-doc_count-field",
    "relUrl": "/im-plugin/index-rollups/index/#the-doc_count-field"
  },"31": {
    "doc": "Index rollups",
    "title": "Query string queries",
    "content": "To take advantage of shorter and easier to write strings in Query DSL, you can use query strings to simplify search queries in rollup indexes. To use query strings, add the following fields to your rollup search request. \"query\": { \"query_string\": { \"query\": \"field_name:field_value\" } } . The following example uses a query string with a * wildcard operator to search inside a rollup index called my_server_logs_rollup. GET my_server_logs_rollup/_search { \"size\": 0, \"query\": { \"query_string\": { \"query\": \"email* OR inventory\", \"default_field\": \"service_name\" } }, \"aggs\": { \"service_name\": { \"terms\": { \"field\": \"service_name\" }, \"aggs\": { \"region\": { \"terms\": { \"field\": \"region\" }, \"aggs\": { \"average quantity\": { \"avg\": { \"field\": \"cpu_usage\" } } } } } } } } . For more information on which parameters are supported in query strings, see Advanced filter options. ",
    "url": "http://localhost:4000/im-plugin/index-rollups/index/#query-string-queries",
    "relUrl": "/im-plugin/index-rollups/index/#query-string-queries"
  },"32": {
    "doc": "Index rollups",
    "title": "Dynamic target index",
    "content": "In ISM rollup, the target_index field may contain a template that is compiled at the time of each rollup indexing. For example, if you specify the target_index field as rollup_ndx-{{ctx.source_index}}, the source index log-000001 will roll up into a target index rollup_ndx-log-000001. This allows you to roll up data into multiple time-based indices, with one rollup job created for each source index. The source_index parameter in {{ctx.source_index}} cannot contain wildcards. ",
    "url": "http://localhost:4000/im-plugin/index-rollups/index/#dynamic-target-index",
    "relUrl": "/im-plugin/index-rollups/index/#dynamic-target-index"
  },"33": {
    "doc": "Index rollups",
    "title": "Searching multiple rollup indices",
    "content": "When data is rolled up into multiple target indices, you can run one search across all of the rollup indices. To search multiple target indices that have the same rollup, specify the index names as a comma-separated list or a wildcard pattern. For example, with target_index as rollup_ndx-{{ctx.source_index}} and source indices that start with log, specify the rollup_ndx-log* pattern. Or, to search for rolled up log-000001 and log-000002 indices, specify the rollup_ndx-log-000001,rollup_ndx-log-000002 list. You cannot search a mix of rollup and non-rollup indices with the same query. ",
    "url": "http://localhost:4000/im-plugin/index-rollups/index/#searching-multiple-rollup-indices",
    "relUrl": "/im-plugin/index-rollups/index/#searching-multiple-rollup-indices"
  },"34": {
    "doc": "Index rollups",
    "title": "Example",
    "content": "The following example demonstrates the doc_count field, dynamic index names, and searching multiple rollup indices with the same rollup. Step 1: Add an index template for ISM to manage the rolling over of the indices aliased by log. PUT _index_template/ism_rollover { \"index_patterns\": [\"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } . Step 2: Set up an ISM rollover policy to roll over any index whose name starts with log* after one document is uploaded to it, and then roll up the individual backing index. The target index name is dynamically generated from the source index name by prepending the string rollup_ndx- to the source index name. PUT _plugins/_ism/policies/rollover_policy { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } } ], \"transitions\": [ { \"state_name\": \"rp\" } ] }, { \"name\": \"rp\", \"actions\": [ { \"rollup\": { \"ism_rollup\": { \"target_index\": \"rollup_ndx-{{ctx.source_index}}\", \"description\": \"Example rollup job\", \"page_size\": 200, \"dimensions\": [ { \"date_histogram\": { \"source_field\": \"ts\", \"fixed_interval\": \"60m\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"message.keyword\" } } ], \"metrics\": [ { \"source_field\": \"msg_size\", \"metrics\": [ { \"sum\": {} } ] } ] } } } ], \"transitions\": [] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . Step 3: Create an index named log-000001 and set up an alias log for it. PUT log-000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } . Step 4: Index four documents into the index created above. Two of the documents have the message “Success”, and two have the message “Error”. POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T09:28:48-04:00\", \"message\": \"Success\", \"msg_size\": 10 } . POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T10:06:25-04:00\", \"message\": \"Error\", \"msg_size\": 20 } . POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T10:23:54-04:00\", \"message\": \"Error\", \"msg_size\": 30 } . POST log/_doc?refresh=true { \"ts\" : \"2022-08-26T10:53:41-04:00\", \"message\": \"Success\", \"msg_size\": 40 } . Once you index the first document, the rollover action is executed. This action creates the index log-000002 with rollover_policy attached to it. Then the rollup action is executed, which creates the rollup index rollup_ndx-log-000001. To monitor the status of rollover and rollup index creation, you can use the ISM explain API: GET _plugins/_ism/explain . Step 5: Search the rollup index. GET rollup_ndx-log-*/_search { \"size\": 0, \"query\": { \"match_all\": {} }, \"aggregations\": { \"message_numbers\": { \"terms\": { \"field\": \"message.keyword\" }, \"aggs\": { \"per_message\": { \"terms\": { \"field\": \"message.keyword\" }, \"aggregations\": { \"sum_message\": { \"sum\": { \"field\": \"msg_size\" } } } } } } } } . The response contains two buckets, “Error” and “Success”, and the document count for each bucket is 2: . { \"took\" : 30, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 4, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] }, \"aggregations\" : { \"message_numbers\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Success\", \"doc_count\" : 2, \"per_message\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Success\", \"doc_count\" : 2, \"sum_message\" : { \"value\" : 50.0 } } ] } }, { \"key\" : \"Error\", \"doc_count\" : 2, \"per_message\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Error\", \"doc_count\" : 2, \"sum_message\" : { \"value\" : 50.0 } } ] } } ] } } } . ",
    "url": "http://localhost:4000/im-plugin/index-rollups/index/#example",
    "relUrl": "/im-plugin/index-rollups/index/#example"
  },"35": {
    "doc": "Index rollups API",
    "title": "Index rollups API",
    "content": "Use the index rollup operations to programmatically work with index rollup jobs. . | Create or update an index rollup job | Get an index rollup job | Delete an index rollup job | Start or stop an index rollup job | Explain an index rollup job | . ",
    "url": "http://localhost:4000/im-plugin/index-rollups/rollup-api/",
    "relUrl": "/im-plugin/index-rollups/rollup-api/"
  },"36": {
    "doc": "Index rollups API",
    "title": "Create or update an index rollup job",
    "content": "Introduced 1.0 . Creates or updates an index rollup job. You must provide the seq_no and primary_term parameters. Request . PUT _plugins/_rollup/jobs/&lt;rollup_id&gt; // Create PUT _plugins/_rollup/jobs/&lt;rollup_id&gt;?if_seq_no=1&amp;if_primary_term=1 // Update { \"rollup\": { \"source_index\": \"nyc-taxi-data\", \"target_index\": \"rollup-nyc-taxi-data\", \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Days\" } }, \"description\": \"Example rollup job\", \"enabled\": true, \"page_size\": 200, \"delay\": 0, \"roles\": [ \"rollup_all\", \"nyc_taxi_all\", \"example_rollup_index_all\" ], \"continuous\": false, \"dimensions\": { \"date_histogram\": { \"source_field\": \"tpep_pickup_datetime\", \"fixed_interval\": \"1h\", \"timezone\": \"America/Los_Angeles\" }, \"terms\": { \"source_field\": \"PULocationID\" }, \"metrics\": [ { \"source_field\": \"passenger_count\", \"metrics\": [ { \"avg\": {} }, { \"sum\": {} }, { \"max\": {} }, { \"min\": {} }, { \"value_count\": {} } ] } ] } } } . You can specify the following options. | Options | Description | Type | Required | . | source_index | The name of the detector. | String | Yes | . | target_index | Specify the target index that the rolled up data is ingested into. You can either create a new target index or use an existing index. The target index cannot be a combination of raw and rolled up data. This field supports dynamically generated index names like rollup_{{ctx.source_index}}, where source_index cannot contain wildcards. | String | Yes | . | schedule | Schedule of the index rollup job which can be an interval or a cron expression. | Object | Yes | . | schedule.interval | Specify the frequency of execution of the rollup job. | Object | No | . | schedule.interval.start_time | Start time of the interval. | Timestamp | Yes | . | schedule.interval.period | Define the interval period. | String | Yes | . | schedule.interval.unit | Specify the time unit of the interval. | String | Yes | . | schedule.interval.cron | Optionally, specify a cron expression to define therollup frequency. | List | No | . | schedule.interval.cron.expression | Specify a Unix cron expression. | String | Yes | . | schedule.interval.cron.timezone | Specify timezones as defined by the IANA Time Zone Database. Defaults to UTC. | String | No | . | description | Optionally, describe the rollup job. | String | No | . | enabled | When true, the index rollup job is scheduled. Default is true. | Boolean | Yes | . | continuous | Specify whether or not the index rollup job continuously rolls up data forever or just executes over the current data set once and stops. Default is false. | Boolean | Yes | . | error_notification | Set up a Mustache message template sent for error notifications. For example, if an index rollup job fails, the system sends a message to a Slack channel. | Object | No | . | page_size | Specify the number of buckets to paginate through at a time while rolling up. | Number | Yes | . | delay | The number of milliseconds to delay execution of the index rollup job. | Long | No | . | dimensions | Specify aggregations to create dimensions for the roll up time window. | Object | Yes | . | dimensions.date_histogram | Specify either fixed_interval or calendar_interval, but not both. Either one limits what you can query in the target index. | Object | No | . | dimensions.date_histogram.fixed_interval | Specify the fixed interval for aggregations in milliseconds, seconds, minutes, hours, or days. | String | No | . | dimensions.date_histogram.calendar_interval | Specify the calendar interval for aggregations in minutes, hours, days, weeks, months, quarters, or years. | String | No | . | dimensions.date_histogram.field | Specify the date field used in date histogram aggregation. | String | No | . | dimensions.date_histogram.timezone | Specify the timezones as defined by the IANA Time Zone Database. The default is UTC. | String | No | . | dimensions.terms | Specify the term aggregations that you want to roll up. | Object | No | . | dimensions.terms.fields | Specify terms aggregation for compatible fields. | Object | No | . | dimensions.histogram | Specify the histogram aggregations that you want to roll up. | Object | No | . | dimensions.histogram.field | Add a field for histogram aggregations. | String | Yes | . | dimensions.histogram.interval | Specify the histogram aggregation interval for the field. | Long | Yes | . | dimensions.metrics | Specify a list of objects that represent the fields and metrics that you want to calculate. | Nested object | No | . | dimensions.metrics.field | Specify the field that you want to perform metric aggregations on. | String | No | . | dimensions.metrics.field.metrics | Specify the metric aggregations you want to calculate for the field. | Multiple strings | No | . Sample response . { \"_id\": \"rollup_id\", \"_seqNo\": 1, \"_primaryTerm\": 1, \"rollup\": { ... } } . ",
    "url": "http://localhost:4000/im-plugin/index-rollups/rollup-api/#create-or-update-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#create-or-update-an-index-rollup-job"
  },"37": {
    "doc": "Index rollups API",
    "title": "Get an index rollup job",
    "content": "Introduced 1.0 . Returns all information about an index rollup job based on the rollup_id. Request . GET _plugins/_rollup/jobs/&lt;rollup_id&gt; . Sample response . { \"_id\": \"my_rollup\", \"_seqNo\": 1, \"_primaryTerm\": 1, \"rollup\": { ... } } . ",
    "url": "http://localhost:4000/im-plugin/index-rollups/rollup-api/#get-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#get-an-index-rollup-job"
  },"38": {
    "doc": "Index rollups API",
    "title": "Delete an index rollup job",
    "content": "Introduced 1.0 . Deletes an index rollup job based on the rollup_id. Request . DELETE _plugins/_rollup/jobs/&lt;rollup_id&gt; . Sample response . 200 OK . ",
    "url": "http://localhost:4000/im-plugin/index-rollups/rollup-api/#delete-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#delete-an-index-rollup-job"
  },"39": {
    "doc": "Index rollups API",
    "title": "Start or stop an index rollup job",
    "content": "Introduced 1.0 . Start or stop an index rollup job. Request . POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_start POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_stop . Sample response . 200 OK . ",
    "url": "http://localhost:4000/im-plugin/index-rollups/rollup-api/#start-or-stop-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#start-or-stop-an-index-rollup-job"
  },"40": {
    "doc": "Index rollups API",
    "title": "Explain an index rollup job",
    "content": "Introduced 1.0 . Returns detailed metadata information about the index rollup job and its current progress. Request . GET _plugins/_rollup/jobs/&lt;rollup_id&gt;/_explain . Sample response . { \"example_rollup\": { \"rollup_id\": \"example_rollup\", \"last_updated_time\": 1602014281, \"continuous\": { \"next_window_start_time\": 1602055591, \"next_window_end_time\": 1602075591 }, \"status\": \"running\", \"failure_reason\": null, \"stats\": { \"pages_processed\": 342, \"documents_processed\": 489359, \"rollups_indexed\": 3420, \"index_time_in_ms\": 30495, \"search_time_in_ms\": 584922 } } } . ",
    "url": "http://localhost:4000/im-plugin/index-rollups/rollup-api/#explain-an-index-rollup-job",
    "relUrl": "/im-plugin/index-rollups/rollup-api/#explain-an-index-rollup-job"
  },"41": {
    "doc": "Settings",
    "title": "Index rollup settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases. All settings are available using the OpenSearch _cluster/settings operation. None require a restart, and all can be marked persistent or transient. | Setting | Default | Description | . | plugins.rollup.search.backoff_millis | 1000 milliseconds | The backoff time between retries for failed rollup jobs. | . | plugins.rollup.search.backoff_count | 5 | How many retries the plugin should attempt for failed rollup jobs. | . | plugins.rollup.search.search_all_jobs | false | Whether OpenSearch should return all jobs that match all specified search terms. If disabled, OpenSearch returns just one, as opposed to all, of the jobs that matches the search terms. | . | plugins.rollup.dashboards.enabled | true | Whether rollups are enabled in OpenSearch Dashboards. | . | plugins.rollup.enabled | true | Whether the rollup plugin is enabled. | . | plugins.ingest.backoff_millis | 1000 milliseconds | The backoff time between data ingestions for rollup jobs. | . | plugins.ingest.backoff_count | 5 | How many retries the plugin should attempt for failed ingestions. | . ",
    "url": "http://localhost:4000/im-plugin/index-rollups/settings/#index-rollup-settings",
    "relUrl": "/im-plugin/index-rollups/settings/#index-rollup-settings"
  },"42": {
    "doc": "Settings",
    "title": "Settings",
    "content": " ",
    "url": "http://localhost:4000/im-plugin/index-rollups/settings/",
    "relUrl": "/im-plugin/index-rollups/settings/"
  },"43": {
    "doc": "Index transforms",
    "title": "Index transforms",
    "content": "Whereas index rollup jobs let you reduce data granularity by rolling up old data into condensed indexes, transform jobs let you create a different, summarized view of your data centered around certain fields, so you can visualize or analyze the data in different ways. For example, suppose that you have airline data that’s scattered across multiple fields and categories, and you want to view a summary of the data that’s organized by airline, quarter, and then price. You can use a transform job to create a new, summarized index that’s organized by those specific categories. You can use transform jobs in two ways: . | Use the OpenSearch Dashboards UI to specify the index you want to transform and any optional data filters you want to use to filter the original index. Then select the fields you want to transform and the aggregations to use in the transformation. Finally, define a schedule for your job to follow. | Use the transforms API to specify all the details about your job: the index you want to transform, target groups you want the transformed index to have, any aggregations you want to use to group columns, and a schedule for your job to follow. | . OpenSearch Dashboards provides a detailed summary of the jobs you created and their relevant information, such as associated indexes and job statuses. You can review and edit your job’s details and selections before creation, and even preview a transformed index’s data as you’re choosing which fields to transform. However, you can also use the REST API to create transform jobs and preview transform job results, but you must know all of the necessary settings and parameters to submit them as part of the HTTP request body. Submitting your transform job configurations as JSON scripts offers you more portability, allowing you to share and replicate your transform jobs, which is harder to do using OpenSearch Dashboards. Your use cases will help you decide which method to use to create transform jobs. ",
    "url": "http://localhost:4000/im-plugin/index-transforms/index/",
    "relUrl": "/im-plugin/index-transforms/index/"
  },"44": {
    "doc": "Index transforms",
    "title": "Create a transform job",
    "content": "If you don’t have any data in your cluster, you can use the sample flight data within OpenSearch Dashboards to try out transform jobs. Otherwise, after launching OpenSearch Dashboards, choose Index Management. Select Transform Jobs, and choose Create Transform Job. Step 1: Choose indexes . | In the Job name and description section, specify a name and an optional description for your job. | In the Indices section, select the source and target index. You can either select an existing target index or create a new one by entering a name for your new index. If you want to transform just a subset of your source index, choose Edit data filter, and use the OpenSearch query DSL to specify a subset of your source index. For more information about the OpenSearch query DSL, see query DSL. | Choose Next. | . Step 2: Select fields to transform . After specifying the indexes, you can select the fields you want to use in your transform job, as well as whether to use groupings or aggregations. You can use groupings to place your data into separate buckets in your transformed index. For example, if you want to group all of the airport destinations within the sample flight data, you can group the DestAirportID field into a target field of DestAirportID_terms field, and you can find the grouped airport IDs in your transformed index after the transform job finishes. On the other hand, aggregations let you perform simple calculations. For example, you can include an aggregation in your transform job to define a new field of sum_of_total_ticket_price that calculates the sum of all airplane tickets, and then analyze the newly summer data within your transformed index. | In the data table, select the fields you want to transform and expand the drop-down menu within the column header to choose the grouping or aggregation you want to use. Currently, transform jobs support histogram, date_histogram, and terms groupings. For more information about groupings, see Bucket Aggregations. In terms of aggregations, you can select from sum, avg, max, min, value_count, percentiles, and scripted_metric. For more information about aggregations, see Metric Aggregations. | Repeat step 1 for any other fields that you want to transform. | After selecting the fields that you want to transform and verifying the transformation, choose Next. | . Step 3: Specify a schedule . You can configure transform jobs to run once or multiple times on a schedule. Transform jobs are enabled by default. | Choose whether the job should be continuous. Continuous jobs execute at each transform execution interval and incrementally transform newly modified buckets, which can include new data added to the source indexes. Non-continuous jobs execute only once. | For transformation execution interval, specify a transform interval in minutes, hours, or days. This interval dicatates how often continuous jobs should execute, and non-continuous jobs execute once after the interval elapses. | Under Advanced, specify an optional amount for Pages per execution. A larger number means more data is processed in each search request, but also uses more memory and causes higher latency. Exceeding allowed memory limits can cause exceptions and errors to occur. | Choose Next. | . Step 4: Review and confirm details . After confirming your transform job’s details are correct, choose Create Transform Job. If you want to edit any part of the job, choose Edit of the section you want to change, and make the necessary changes. You can’t change aggregations or groupings after creating a job. Step 5: Search through the transformed index. Once the transform job finishes, you can use the _search API operation to search the target index. GET &lt;target_index&gt;/_search . For example, after running a transform job that transforms the flight data based on a DestAirportID field, you can run the following request that returns all of the fields that have a value of SFO. Sample Request . GET finished_flight_job/_search { \"query\": { \"match\": { \"DestAirportID_terms\" : \"SFO\" } } } . Sample Response . { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 5, \"successful\" : 5, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 4, \"relation\" : \"eq\" }, \"max_score\" : 3.845883, \"hits\" : [ { \"_index\" : \"finished_flight_job\", \"_id\" : \"dSNKGb8U3OJOmC4RqVCi1Q\", \"_score\" : 3.845883, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 14, \"Carrier_terms\" : \"Dashboards Airlines\", \"DestAirportID_terms\" : \"SFO\" } }, { \"_index\" : \"finished_flight_job\", \"_id\" : \"_D7oqOy7drx9E-MG96U5RA\", \"_score\" : 3.845883, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 14, \"Carrier_terms\" : \"Logstash Airways\", \"DestAirportID_terms\" : \"SFO\" } }, { \"_index\" : \"finished_flight_job\", \"_id\" : \"YuZ8tOt1OsBA54e84WuAEw\", \"_score\" : 3.6988301, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 11, \"Carrier_terms\" : \"ES-Air\", \"DestAirportID_terms\" : \"SFO\" } }, { \"_index\" : \"finished_flight_job\", \"_id\" : \"W_-e7bVmH6eu8veJeK8ZxQ\", \"_score\" : 3.6988301, \"_source\" : { \"transform._id\" : \"sample_flight_job\", \"transform._doc_count\" : 10, \"Carrier_terms\" : \"JetBeats\", \"DestAirportID_terms\" : \"SFO\" } } ] } } . ",
    "url": "http://localhost:4000/im-plugin/index-transforms/index/#create-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/index/#create-a-transform-job"
  },"45": {
    "doc": "Transforms APIs",
    "title": "Transforms APIs",
    "content": "Aside from using OpenSearch Dashboards, you can also use the REST API to create, start, stop, and complete other operations relative to transform jobs. | Create a transform job . | Request format | Path parameters | Request body fields | . | Update a transform job . | Request format | Query parameters | Request body fields | . | Get a transform job’s details . | Request format | Query parameters | . | Start a transform job . | Request format | . | Stop a transform job . | Request format | . | Get the status of a transform job . | Request format | . | Preview a transform job’s results | Delete a transform job . | Request format | . | . ",
    "url": "http://localhost:4000/im-plugin/index-transforms/transforms-apis/",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/"
  },"46": {
    "doc": "Transforms APIs",
    "title": "Create a transform job",
    "content": "Introduced 1.0 . Creates a transform job. Request format . PUT _plugins/_transform/&lt;transform_id&gt; . Path parameters . | Parameter | Data Type | Description | . | transform_id | String | Transform ID | . Request body fields . You can specify the following options in the HTTP request body: . | Option | Data Type | Description | Required | . | enabled | Boolean | If true, the transform job is enabled at creation. | No | . | continuous | Boolean | Specifies whether the transform job should be continuous. Continuous jobs execute every time they are scheduled according to the schedule field and run based off of newly transformed buckets as well as any new data added to source indexes. Non-continuous jobs execute only once. Default is false. | No | . | schedule | Object | The schedule for the transform job. | Yes | . | start_time | Integer | The Unix epoch time of the transform job’s start time. | Yes | . | description | String | Describes the transform job. | No | . | metadata_id | String | Any metadata to be associated with the transform job. | No | . | source_index | String | The source index containing the data to be transformed. | Yes | . | target_index | String | The target index the newly transformed data is added to. You can create a new index or update an existing one. | Yes | . | data_selection_query | Object | The query DSL to use to filter a subset of the source index for the transform job. See query domain-specific language(DSL) for more information. | Yes | . | page_size | Integer | The number of buckets IM processes and indexes concurrently. A higher number results in better performance, but it requires more memory. If your machine runs out of memory, Index Management (IM) automatically adjusts this field and retries until the operation succeeds. | Yes | . | groups | Array | Specifies the grouping(s) to use in the transform job. Supported groups are terms, histogram, and date_histogram. For more information, see Bucket Aggregations. | Yes if not using aggregations. | . | source_field | String | The field(s) to transform. | Yes | . | aggregations | Object | The aggregations to use in the transform job. Supported aggregations are sum, max, min, value_count, avg, scripted_metric, and percentiles. For more information, see Metric Aggregations. | Yes if not using groups. | . Sample Request . The following request creates a transform job with the id sample: . PUT _plugins/_transform/sample { \"transform\": { \"enabled\": true, \"continuous\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . Sample Response . { \"_id\": \"sample\", \"_version\": 7, \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . ",
    "url": "http://localhost:4000/im-plugin/index-transforms/transforms-apis/#create-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#create-a-transform-job"
  },"47": {
    "doc": "Transforms APIs",
    "title": "Update a transform job",
    "content": "Introduced 1.0 . Updates the transform job if transform_id already exists. For this request you must specify the sequence number and primary term of the transform to be updated. To get these, use the Get a transform job’s details API call. Request format . PUT _plugins/_transform/&lt;transform_id&gt;?if_seq_no=&lt;seq_no&gt;&amp;if_primary_term=&lt;primary_term&gt; . Query parameters . The update operation supports the following query parameters: . | Parameter | Description | Required | . | seq_no | Only perform the transform operation if the last operation that changed the transform job has the specified sequence number. | Yes | . | primary_term | Only perform the transform operation if the last operation that changed the transform job has the specified sequence term. | Yes | . Request body fields . You can update the following fields. | Option | Data Type | Description | . | schedule | Object | The schedule for the transform job. Contains the fields interval.start_time, interval.period, and interval.unit. | . | start_time | Integer | The Unix epoch start time of the transform job. | . | period | Integer | How often to execute the transform job. | . | unit | String | The unit of time associated with the execution period. Available options are Minutes, Hours, and Days. | . | description | Integer | Describes the transform job. | . | page_size | Integer | The number of buckets IM processes and indexes concurrently. A higher number results in better performance, but it requires more memory. If your machine runs out of memory, IM automatically adjusts this field and retries until the operation succeeds. | . Sample Request . The following request updates a transform job with the id sample, sequence number 13, and primary term 1: . PUT _plugins/_transform/sample?if_seq_no=13&amp;if_primary_term=1 { \"transform\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . Sample Response . PUT _plugins/_transform/sample?if_seq_no=13&amp;if_primary_term=1 { \"transform\": { \"enabled\": true, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . ",
    "url": "http://localhost:4000/im-plugin/index-transforms/transforms-apis/#update-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#update-a-transform-job"
  },"48": {
    "doc": "Transforms APIs",
    "title": "Get a transform job’s details",
    "content": "Introduced 1.0 . Returns a transform job’s details. Request format . GET _plugins/_transform/&lt;transform_id&gt; . Sample Request . The following request returns the details of the transform job with the id sample: . GET _plugins/_transform/sample . Sample Response . { \"_id\": \"sample\", \"_version\": 7, \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . You can also get details of all transform jobs by omitting transform_id. Sample Request . The following request returns the details of all transform jobs: . GET _plugins/_transform/ . Sample Response . { \"total_transforms\": 1, \"transforms\": [ { \"_id\": \"sample\", \"_seq_no\": 13, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample\", \"schema_version\": 7, \"continuous\": true, \"schedule\": { \"interval\": { \"start_time\": 1621467964243, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": null, \"updated_at\": 1621467964243, \"enabled\": true, \"enabled_at\": 1621467964243, \"description\": \"Sample transform job\", \"source_index\": \"sample_index\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } ] } . Query parameters . You can specify the following GET API operation’s query parameters to filter the results. | Parameter | Description | Required | . | from | The starting transform to return. Default is 0. | No | . | size | Specifies the number of transforms to return. Default is 10. | No | . | search | The search term to use to filter results. | No | . | sortField | The field to sort results with. | No | . | sortDirection | Specifies the direction to sort results in. Can be ASC or DESC. Default is ASC. | No | . Sample Request . The following request returns two results starting from transform 8: . GET _plugins/_transform?size=2&amp;from=8 . Sample Response . { \"total_transforms\": 18, \"transforms\": [ { \"_id\": \"sample8\", \"_seq_no\": 93, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample8\", \"schema_version\": 7, \"schedule\": { \"interval\": { \"start_time\": 1622063596812, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": \"y4hFAB2ZURQ2dzY7BAMxWA\", \"updated_at\": 1622063657233, \"enabled\": false, \"enabled_at\": null, \"description\": \"Sample transform job\", \"source_index\": \"sample_index3\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target3\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } }, { \"_id\": \"sample9\", \"_seq_no\": 98, \"_primary_term\": 1, \"transform\": { \"transform_id\": \"sample9\", \"schema_version\": 7, \"schedule\": { \"interval\": { \"start_time\": 1622063598065, \"period\": 1, \"unit\": \"Minutes\" } }, \"metadata_id\": \"x8tCIiYMTE3veSbIJkit5A\", \"updated_at\": 1622063658388, \"enabled\": false, \"enabled_at\": null, \"description\": \"Sample transform job\", \"source_index\": \"sample_index4\", \"data_selection_query\": { \"match_all\": { \"boost\": 1.0 } }, \"target_index\": \"sample_target4\", \"roles\": [], \"page_size\": 1, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } ] } . ",
    "url": "http://localhost:4000/im-plugin/index-transforms/transforms-apis/#get-a-transform-jobs-details",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#get-a-transform-jobs-details"
  },"49": {
    "doc": "Transforms APIs",
    "title": "Start a transform job",
    "content": "Introduced 1.0 . Transform jobs created using the API are automatically enabled, but if you ever need to enable a job, you can use the start API operation. Request format . POST _plugins/_transform/&lt;transform_id&gt;/_start . Sample Request . The following request starts the transform job with the ID sample: . POST _plugins/_transform/sample/_start . Sample Response . { \"acknowledged\": true } . ",
    "url": "http://localhost:4000/im-plugin/index-transforms/transforms-apis/#start-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#start-a-transform-job"
  },"50": {
    "doc": "Transforms APIs",
    "title": "Stop a transform job",
    "content": "Introduced 1.0 . Stops a transform job. Request format . POST _plugins/_transform/&lt;transform_id&gt;/_stop . Sample Request . The following request stops the transform job with the ID sample: . POST _plugins/_transform/sample/_stop . Sample Response . { \"acknowledged\": true } . ",
    "url": "http://localhost:4000/im-plugin/index-transforms/transforms-apis/#stop-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#stop-a-transform-job"
  },"51": {
    "doc": "Transforms APIs",
    "title": "Get the status of a transform job",
    "content": "Introduced 1.0 . Returns the status and metadata of a transform job. Request format . GET _plugins/_transform/&lt;transform_id&gt;/_explain . Sample Request . The following request returns the details of the transform job with the ID sample: . GET _plugins/_transform/sample/_explain . Sample Response . { \"sample\": { \"metadata_id\": \"PzmjweME5xbgkenl9UpsYw\", \"transform_metadata\": { \"continuous_stats\": { \"last_timestamp\": 1621883525672, \"documents_behind\": { \"sample_index\": 72 } }, \"transform_id\": \"sample\", \"last_updated_at\": 1621883525873, \"status\": \"finished\", \"failure_reason\": \"null\", \"stats\": { \"pages_processed\": 0, \"documents_processed\": 0, \"documents_indexed\": 0, \"index_time_in_millis\": 0, \"search_time_in_millis\": 0 } } } } . ",
    "url": "http://localhost:4000/im-plugin/index-transforms/transforms-apis/#get-the-status-of-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#get-the-status-of-a-transform-job"
  },"52": {
    "doc": "Transforms APIs",
    "title": "Preview a transform job’s results",
    "content": "Introduced 1.0 . Returns a preview of what a transformed index would look like. Sample Request . POST _plugins/_transform/_preview { \"transform\": { \"enabled\": false, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"Minutes\", \"start_time\": 1602100553 } }, \"description\": \"test transform\", \"source_index\": \"sample_index\", \"target_index\": \"sample_target\", \"data_selection_query\": { \"match_all\": {} }, \"page_size\": 10, \"groups\": [ { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day\" } } ], \"aggregations\": { \"quantity\": { \"sum\": { \"field\": \"total_quantity\" } } } } } . Sample Response . { \"documents\" : [ { \"quantity\" : 862.0, \"gender\" : \"FEMALE\", \"day\" : \"Friday\" }, { \"quantity\" : 682.0, \"gender\" : \"FEMALE\", \"day\" : \"Monday\" }, { \"quantity\" : 772.0, \"gender\" : \"FEMALE\", \"day\" : \"Saturday\" }, { \"quantity\" : 669.0, \"gender\" : \"FEMALE\", \"day\" : \"Sunday\" }, { \"quantity\" : 887.0, \"gender\" : \"FEMALE\", \"day\" : \"Thursday\" } ] } . ",
    "url": "http://localhost:4000/im-plugin/index-transforms/transforms-apis/#preview-a-transform-jobs-results",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#preview-a-transform-jobs-results"
  },"53": {
    "doc": "Transforms APIs",
    "title": "Delete a transform job",
    "content": "Introduced 1.0 . Deletes a transform job. This operation does not delete the source or target indexes. Request format . DELETE _plugins/_transform/&lt;transform_id&gt; . Sample Request . The following request deletes the transform job with the ID sample: . DELETE _plugins/_transform/sample . Sample Response . { \"took\": 205, \"errors\": false, \"items\": [ { \"delete\": { \"_index\": \".opensearch-ism-config\", \"_id\": \"sample\", \"_version\": 4, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 1, \"failed\": 0 }, \"_seq_no\": 6, \"_primary_term\": 1, \"status\": 200 } } ] } . ",
    "url": "http://localhost:4000/im-plugin/index-transforms/transforms-apis/#delete-a-transform-job",
    "relUrl": "/im-plugin/index-transforms/transforms-apis/#delete-a-transform-job"
  },"54": {
    "doc": "About Index Management",
    "title": "About Index Management",
    "content": "OpenSearch Dashboards . The Index Management (IM) plugin lets you automate recurring index management activities and reduce storage costs. ",
    "url": "http://localhost:4000/im-plugin/index/",
    "relUrl": "/im-plugin/index/"
  },"55": {
    "doc": "ISM API",
    "title": "ISM API",
    "content": "Use the index state management operations to programmatically work with policies and managed indexes. . | Create policy | Add policy | Update policy | Get policy | Remove policy from index | Update managed index policy | Retry failed index | Explain index | Delete policy | Error prevention validation | . ",
    "url": "http://localhost:4000/im-plugin/ism/api/",
    "relUrl": "/im-plugin/ism/api/"
  },"56": {
    "doc": "ISM API",
    "title": "Create policy",
    "content": "Introduced 1.0 . Creates a policy. Sample request . PUT _plugins/_ism/policies/policy_1 { \"policy\": { \"description\": \"ingesting logs\", \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } . Sample response . { \"_id\": \"policy_1\", \"_version\": 1, \"_primary_term\": 1, \"_seq_no\": 7, \"policy\": { \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990761311, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } } . ",
    "url": "http://localhost:4000/im-plugin/ism/api/#create-policy",
    "relUrl": "/im-plugin/ism/api/#create-policy"
  },"57": {
    "doc": "ISM API",
    "title": "Add policy",
    "content": "Introduced 1.0 . Adds a policy to an index. This operation does not change the policy if the index already has one. Sample request . POST _plugins/_ism/add/index_1 { \"policy_id\": \"policy_1\" } . Sample response . { \"updated_indices\": 1, \"failures\": false, \"failed_indices\": [] } . If you use a wildcard * while adding a policy to an index, the ISM plugin interprets * as all indexes, including system indexes like .opendistro-security, which stores users, roles, and tenants. A delete action in your policy might accidentally delete all user roles and tenants in your cluster. Don’t use the broad * wildcard, and instead add a prefix, such as my-logs*, when specifying indexes with the _ism/add API. ",
    "url": "http://localhost:4000/im-plugin/ism/api/#add-policy",
    "relUrl": "/im-plugin/ism/api/#add-policy"
  },"58": {
    "doc": "ISM API",
    "title": "Update policy",
    "content": "Introduced 1.0 . Updates a policy. Use the seq_no and primary_term parameters to update an existing policy. If these numbers don’t match the existing policy or the policy doesn’t exist, ISM throws an error. It’s possible that the policy currently applied to your index isn’t the most up-to-date policy available. To see what policy is currently applied to your index, see Explain index. To get the most up-to-date version of a policy, see Get policy. Sample request . PUT _plugins/_ism/policies/policy_1?if_seq_no=7&amp;if_primary_term=1 { \"policy\": { \"description\": \"ingesting logs\", \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } . Sample response . { \"_id\": \"policy_1\", \"_version\": 2, \"_primary_term\": 1, \"_seq_no\": 10, \"policy\": { \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990934044, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } } . ",
    "url": "http://localhost:4000/im-plugin/ism/api/#update-policy",
    "relUrl": "/im-plugin/ism/api/#update-policy"
  },"59": {
    "doc": "ISM API",
    "title": "Get policy",
    "content": "Introduced 1.0 . Gets the policy by policy_id. Sample request . GET _plugins/_ism/policies/policy_1 . Sample response . { \"_id\": \"policy_1\", \"_version\": 2, \"_seq_no\": 10, \"_primary_term\": 1, \"policy\": { \"policy_id\": \"policy_1\", \"description\": \"ingesting logs\", \"last_updated_time\": 1577990934044, \"schema_version\": 1, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [ { \"name\": \"ingest\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 5 } } ], \"transitions\": [ { \"state_name\": \"search\" } ] }, { \"name\": \"search\", \"actions\": [], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"5m\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"delete\": {} } ], \"transitions\": [] } ] } } . ",
    "url": "http://localhost:4000/im-plugin/ism/api/#get-policy",
    "relUrl": "/im-plugin/ism/api/#get-policy"
  },"60": {
    "doc": "ISM API",
    "title": "Remove policy from index",
    "content": "Introduced 1.0 . Removes any ISM policy from the index. Sample request . POST _plugins/_ism/remove/index_1 . Sample response . { \"updated_indices\": 1, \"failures\": false, \"failed_indices\": [] } . ",
    "url": "http://localhost:4000/im-plugin/ism/api/#remove-policy-from-index",
    "relUrl": "/im-plugin/ism/api/#remove-policy-from-index"
  },"61": {
    "doc": "ISM API",
    "title": "Update managed index policy",
    "content": "Introduced 1.0 . Updates the managed index policy to a new policy (or to a new version of the policy). You can use an index pattern to update multiple indexes at once. When updating multiple indexes, you might want to include a state filter to only affect certain managed indexes. The change policy filters out all the existing managed indexes and only applies the change to the ones in the state that you specify. You can also explicitly specify the state that the managed index transitions to after the change policy takes effect. A policy change is an asynchronous background process. The changes are queued and are not executed immediately by the background process. This delay in execution protects the currently running managed indexes from being put into a broken state. If the policy you are changing to has only some small configuration changes, then the change takes place immediately. For example, if the policy changes the min_index_age parameter in a rollover condition from 1000d to 100d, this change takes place immediately in its next execution. If the change modifies the state, actions, or the order of actions of the current state the index is in, then the change happens at the end of its current state before transitioning to a new state. In this example, the policy applied on the index_1 index is changed to policy_1, which could either be a completely new policy or an updated version of its existing policy. The process only applies the change if the index is currently in the searches state. After this change in policy takes place, index_1 transitions to the delete state. Sample request . POST _plugins/_ism/change_policy/index_1 { \"policy_id\": \"policy_1\", \"state\": \"delete\", \"include\": [ { \"state\": \"searches\" } ] } . Sample response . { \"updated_indices\": 0, \"failures\": false, \"failed_indices\": [] } . ",
    "url": "http://localhost:4000/im-plugin/ism/api/#update-managed-index-policy",
    "relUrl": "/im-plugin/ism/api/#update-managed-index-policy"
  },"62": {
    "doc": "ISM API",
    "title": "Retry failed index",
    "content": "Introduced 1.0 . Retries the failed action for an index. For the retry call to succeed, ISM must manage the index, and the index must be in a failed state. You can use index patterns (*) to retry multiple failed indexes. Sample request . POST _plugins/_ism/retry/index_1 { \"state\": \"delete\" } . Sample response . { \"updated_indices\": 0, \"failures\": false, \"failed_indices\": [] } . ",
    "url": "http://localhost:4000/im-plugin/ism/api/#retry-failed-index",
    "relUrl": "/im-plugin/ism/api/#retry-failed-index"
  },"63": {
    "doc": "ISM API",
    "title": "Explain index",
    "content": "Introduced 1.0 . Gets the current state of the index. You can use index patterns to get the status of multiple indexes. Sample request . GET _plugins/_ism/explain/index_1 . Sample response . { \"index_1\": { \"index.plugins.index_state_management.policy_id\": \"policy_1\" } } . Optionally, you can add the show_policy parameter to your request’s path to get the policy that is currently applied to your index, which is useful for seeing whether the policy applied to your index is the latest one. To get the most up-to-date policy, see Get Policy API. Sample request . GET _plugins/_ism/explain/index_1?show_policy=true . Sample response . { \"index_1\": { \"index.plugins.index_state_management.policy_id\": \"sample-policy\", \"index.opendistro.index_state_management.policy_id\": \"sample-policy\", \"index\": \"index_1\", \"index_uuid\": \"gCFlS_zcTdih8xyxf3jQ-A\", \"policy_id\": \"sample-policy\", \"enabled\": true, \"policy\": { \"policy_id\": \"sample-policy\", \"description\": \"ingesting logs\", \"last_updated_time\": 1647284980148, \"schema_version\": 13, \"error_notification\": null, \"default_state\": \"ingest\", \"states\": [...], \"ism_template\": null } }, \"total_managed_indices\": 1 } . The plugins.index_state_management.policy_id setting is deprecated starting from ODFE version 1.13.0. We retain this field in the response API for consistency. ",
    "url": "http://localhost:4000/im-plugin/ism/api/#explain-index",
    "relUrl": "/im-plugin/ism/api/#explain-index"
  },"64": {
    "doc": "ISM API",
    "title": "Delete policy",
    "content": "Introduced 1.0 . Deletes the policy by policy_id. Sample request . DELETE _plugins/_ism/policies/policy_1 . Sample response . { \"_index\": \".opendistro-ism-config\", \"_id\": \"policy_1\", \"_version\": 3, \"result\": \"deleted\", \"forced_refresh\": true, \"_shards\": { \"total\": 2, \"successful\": 2, \"failed\": 0 }, \"_seq_no\": 15, \"_primary_term\": 1 } . ",
    "url": "http://localhost:4000/im-plugin/ism/api/#delete-policy",
    "relUrl": "/im-plugin/ism/api/#delete-policy"
  },"65": {
    "doc": "ISM API",
    "title": "Error prevention validation",
    "content": "Introduced 2.4 . ISM allows you to run an action automatically. However, running an action can fail for a variety of reasons. You can use error prevention validation to test an action in order to rule out failures. To enable error prevention validation, set the plugins.index_state_management.validation_service.enabled setting to true: . PUT _cluster/settings { \"persistent\":{ \"plugins.index_state_management.validation_action.enabled\": true } } . Sample response . { \"acknowledged\" : true, \"persistent\" : { \"plugins\" : { \"index_state_management\" : { \"validation_action\" : { \"enabled\" : \"true\" } } } }, \"transient\" : { } } . To check an error prevention validation status and message, pass validate_action=true to the _plugins/_ism/explain endpoint: . GET _plugins/_ism/explain/test-000001?validate_action=true . Sample response . The response contains an additional validate object with a validation message and status: . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false, \"validate\" : { \"validation_message\" : \"Missing rollover_alias index setting [index=test-000001]\", \"validation_status\" : \"re_validating\" } }, \"total_managed_indices\" : 1 } . If you pass validate_action=false or do not pass a validate_action value to the _plugins/_ism/explain endpoint, the response will not contain an error prevention validation status and message: . GET _plugins/_ism/explain/test-000001?validate_action=false . Or: . GET _plugins/_ism/explain/test-000001 . Sample response . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false }, \"total_managed_indices\" : 1 } . ",
    "url": "http://localhost:4000/im-plugin/ism/api/#error-prevention-validation",
    "relUrl": "/im-plugin/ism/api/#error-prevention-validation"
  },"66": {
    "doc": "ISM Error Prevention API",
    "title": "ISM Error Prevention API",
    "content": "The ISM Error Prevention API allows you to enable Index State Management (ISM) error prevention and check the validation status and message. ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/api/",
    "relUrl": "/im-plugin/ism/error-prevention/api/"
  },"67": {
    "doc": "ISM Error Prevention API",
    "title": "Enable error prevention validation",
    "content": "You can configure error prevention validation by setting the plugins.index_state_management.validation_service.enabled parameter. Sample request . PUT _cluster/settings { \"persistent\":{ \"plugins.index_state_management.validation_action.enabled\": true } } . Sample response . { \"acknowledged\" : true, \"persistent\" : { \"plugins\" : { \"index_state_management\" : { \"validation_action\" : { \"enabled\" : \"true\" } } } }, \"transient\" : { } } . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/api/#enable-error-prevention-validation",
    "relUrl": "/im-plugin/ism/error-prevention/api/#enable-error-prevention-validation"
  },"68": {
    "doc": "ISM Error Prevention API",
    "title": "Check validation status and message via the Explain API",
    "content": "Pass the validate_action=true path parameter in the Explain API URI to see the validation status and message. Sample request . GET _plugins/_ism/explain/test-000001?validate_action=true . Sample response . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false, \"validate\" : { \"validation_message\" : \"Missing rollover_alias index setting [index=test-000001]\", \"validation_status\" : \"re_validating\" } }, \"total_managed_indices\" : 1 } . If you pass the parameter without a value or false, then it doesn’t return the validation status and message. Only if you pass validate_action=true will the response will return the validation status and message. Sample request . GET _plugins/_ism/explain/test-000001?validate_action=false --- OR --- GET _plugins/_ism/explain/test-000001 . Sample response . { \"test-000001\" : { \"index.plugins.index_state_management.policy_id\" : \"test_rollover\", \"index.opendistro.index_state_management.policy_id\" : \"test_rollover\", \"index\" : \"test-000001\", \"index_uuid\" : \"CgKsxFmQSIa8dWqpbSJmyA\", \"policy_id\" : \"test_rollover\", \"policy_seq_no\" : -2, \"policy_primary_term\" : 0, \"rolled_over\" : false, \"index_creation_date\" : 1667410460649, \"state\" : { \"name\" : \"rollover\", \"start_time\" : 1667410766045 }, \"action\" : { \"name\" : \"rollover\", \"start_time\" : 1667411127803, \"index\" : 0, \"failed\" : false, \"consumed_retries\" : 0, \"last_retry_time\" : 0 }, \"step\" : { \"name\" : \"attempt_rollover\", \"start_time\" : 1667411127803, \"step_status\" : \"starting\" }, \"retry_info\" : { \"failed\" : true, \"consumed_retries\" : 0 }, \"info\" : { \"message\" : \"Previous action was not able to update IndexMetaData.\" }, \"enabled\" : false }, \"total_managed_indices\" : 1 } . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/api/#check-validation-status-and-message-via-the-explain-api",
    "relUrl": "/im-plugin/ism/error-prevention/api/#check-validation-status-and-message-via-the-explain-api"
  },"69": {
    "doc": "ISM Error Prevention",
    "title": "ISM error prevention",
    "content": "Error prevention validates Index State Management (ISM) actions before they are performed in order to prevent actions from failing. It also outputs additional information from the action validation results in the response of the Index Explain API. Validation rules and troubleshooting of each action are listed in the following sections. . | rollover | delete | force_merge | replica_count | open | read_only | read_write | . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/index/#ism-error-prevention",
    "relUrl": "/im-plugin/ism/error-prevention/index/#ism-error-prevention"
  },"70": {
    "doc": "ISM Error Prevention",
    "title": "rollover",
    "content": "ISM does not perform a rollover action for an index under any of these conditions: . | The index is not the write index. | The index does not have an alias. | The rollover policy does not contain a rollover_alias index setting. | Skipping of a rollover action has occured. | The index has already been rolled over using the alias successfully. | . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/index/#rollover",
    "relUrl": "/im-plugin/ism/error-prevention/index/#rollover"
  },"71": {
    "doc": "ISM Error Prevention",
    "title": "delete",
    "content": "ISM does not perform a delete action for an index under any of these conditions: . | The index does not exist. | The index name is invalid. | The index is the write index for a data stream. | . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/index/#delete",
    "relUrl": "/im-plugin/ism/error-prevention/index/#delete"
  },"72": {
    "doc": "ISM Error Prevention",
    "title": "force_merge",
    "content": "ISM does not perform a force_merge action for an index if its dataset is too large and exceeds the threshold. ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/index/#force_merge",
    "relUrl": "/im-plugin/ism/error-prevention/index/#force_merge"
  },"73": {
    "doc": "ISM Error Prevention",
    "title": "replica_count",
    "content": "ISM does not perform a replica_count action for an index under any of these conditions: . | The amount of data exceeds the threshold. | The number of shards exceeds the maximum. | . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/index/#replica_count",
    "relUrl": "/im-plugin/ism/error-prevention/index/#replica_count"
  },"74": {
    "doc": "ISM Error Prevention",
    "title": "open",
    "content": "ISM does not perform an open action for an index under any of these conditions: . | The index is blocked. | The number of shards exceeds the maximum. | . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/index/#open",
    "relUrl": "/im-plugin/ism/error-prevention/index/#open"
  },"75": {
    "doc": "ISM Error Prevention",
    "title": "read_only",
    "content": "ISM does not perform a read_only action for an index under any of these conditions: . | The index is blocked. | The amount of data exceeds the threshold. | . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/index/#read_only",
    "relUrl": "/im-plugin/ism/error-prevention/index/#read_only"
  },"76": {
    "doc": "ISM Error Prevention",
    "title": "read_write",
    "content": "ISM does not perform a read_write action for an index if the index is blocked. ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/index/#read_write",
    "relUrl": "/im-plugin/ism/error-prevention/index/#read_write"
  },"77": {
    "doc": "ISM Error Prevention",
    "title": "ISM Error Prevention",
    "content": " ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/index/",
    "relUrl": "/im-plugin/ism/error-prevention/index/"
  },"78": {
    "doc": "ISM Error Prevention resolutions",
    "title": "ISM error prevention resolutions",
    "content": "Resolutions of errors for each validation rule action are listed in the following sections. . | The index is not the write index | The index does not have an alias | Skipping rollover action is true | This index has already been rolled over successfully | The rollover policy misses rollover_alias index setting | Data too large and exceeding the threshold | Maximum shards exceeded | The index is a write index for some data stream | The index is blocked | . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/#ism-error-prevention-resolutions",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#ism-error-prevention-resolutions"
  },"79": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index is not the write index",
    "content": "To confirm that the index is a write index, run the following request: . GET &lt;index&gt;/_alias?pretty . If the response does not contain \"is_write_index\" : true, the index is not a write index. The following example confirms that the index is a write index: . { \"&lt;index&gt;\" : { \"aliases\" : { \"&lt;index_alias&gt;\" : { \"is_write_index\" : true } } } } . To set the index as a write index, run the following request: . PUT &lt;index&gt; { \"aliases\": { \"&lt;index_alias&gt;\" : { \"is_write_index\" : true } } } . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/#the-index-is-not-the-write-index",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-is-not-the-write-index"
  },"80": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index does not have an alias",
    "content": "If the index does not have an alias, you can add one by running the following request: . POST _aliases { \"actions\": [ { \"add\": { \"index\": \"&lt;target_index&gt;\", \"alias\": \"&lt;index_alias&gt;\" } } ] } . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/#the-index-does-not-have-an-alias",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-does-not-have-an-alias"
  },"81": {
    "doc": "ISM Error Prevention resolutions",
    "title": "Skipping rollover action is true",
    "content": "In the event that skipping a rollover action occurs, run the following request: . GET &lt;target_index&gt;/_settings?pretty . If you receive the response in the first example, you can reset it by running the request in the second example: . { \"index\": { \"opendistro.index_state_management.rollover_skip\": true } } . PUT &lt;target_index&gt;/_settings { \"index\": { \"index_state_management.rollover_skip\": false } } . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/#skipping-rollover-action-is-true",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#skipping-rollover-action-is-true"
  },"82": {
    "doc": "ISM Error Prevention resolutions",
    "title": "This index has already been rolled over successfully",
    "content": "Remove the rollover policy from the index to prevent this error from reoccurring. ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/#this-index-has-already-been-rolled-over-successfully",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#this-index-has-already-been-rolled-over-successfully"
  },"83": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The rollover policy misses rollover_alias index setting",
    "content": "Add a rollover_alias index setting to the rollover policy to resolve this issue. Run the following request: . PUT _index_template/ism_rollover { \"index_patterns\": [\"&lt;index_patterns_in_rollover_policy&gt;\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"&lt;rollover_alias&gt;\" } } } . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/#the-rollover-policy-misses-rollover_alias-index-setting",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-rollover-policy-misses-rollover_alias-index-setting"
  },"84": {
    "doc": "ISM Error Prevention resolutions",
    "title": "Data too large and exceeding the threshold",
    "content": "Check the JVM information and increase the heap memory. ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/#data-too-large-and-exceeding-the-threshold",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#data-too-large-and-exceeding-the-threshold"
  },"85": {
    "doc": "ISM Error Prevention resolutions",
    "title": "Maximum shards exceeded",
    "content": "The shard limit per node, or per index, causes this issue to occur. Check whether there is a total_shards_per_node limit by running the following request: . GET /_cluster/settings . If the response contains total_shards_per_node, increase its value temporarily by running the following request: . PUT _cluster/settings { \"transient\":{ \"cluster.routing.allocation.total_shards_per_node\":100 } } . To check whether there is a shard limit for an index, run the following request: . GET &lt;index&gt;/_settings/index.routing- . If the response contains the setting in the first example, increase its value or set it to -1 for unlimited shards, as shown in the second example: . \"index\" : { \"routing\" : { \"allocation\" : { \"total_shards_per_node\" : \"10\" } } } . PUT &lt;index&gt;/_settings {\"index.routing.allocation.total_shards_per_node\":-1} . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/#maximum-shards-exceeded",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#maximum-shards-exceeded"
  },"86": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index is a write index for some data stream",
    "content": "If you still want to delete the index, check your data stream settings and change the write index. ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/#the-index-is-a-write-index-for-some-data-stream",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-is-a-write-index-for-some-data-stream"
  },"87": {
    "doc": "ISM Error Prevention resolutions",
    "title": "The index is blocked",
    "content": "Generally, the index is blocked because disk usage has exceeded the flood-stage watermark and the index has a read-only-allow-delete block. To resolve this issue, you can: . | Remove the -index.blocks.read_only_allow_delete- parameter. | Temporarily increase the disk watermarks. | Temporarily disable the disk allocation threshold. | . To prevent the issue from reoccurring, it is better to reduce the usage of the disk by increasing disk space, adding new nodes, or removing data or indexes that are no longer needed. Remove -index.blocks.read_only_allow_delete- by running the following request: . PUT &lt;index&gt;/_settings { \"index.blocks.read_only_allow_delete\": null } . Increase the low disk watermarks by running the following request: . PUT _cluster/settings { \"transient\": { \"cluster\": { \"routing\": { \"allocation\": { \"disk\": { \"watermark\": { \"low\": \"25.0gb\" } } } } } } } . Disable the disk allocation threshold by running the following request: . PUT _cluster/settings { \"transient\": { \"cluster\": { \"routing\": { \"allocation\": { \"disk\": { \"threshold_enabled\" : false } } } } } } . ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/#the-index-is-blocked",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/#the-index-is-blocked"
  },"88": {
    "doc": "ISM Error Prevention resolutions",
    "title": "ISM Error Prevention resolutions",
    "content": " ",
    "url": "http://localhost:4000/im-plugin/ism/error-prevention/resolutions/",
    "relUrl": "/im-plugin/ism/error-prevention/resolutions/"
  },"89": {
    "doc": "Index State Management",
    "title": "Index State Management",
    "content": "OpenSearch Dashboards . If you analyze time-series data, you likely prioritize new data over old data. You might periodically perform certain operations on older indexes, such as reducing replica count or deleting them. Index State Management (ISM) is a plugin that lets you automate these periodic, administrative operations by triggering them based on changes in the index age, index size, or number of documents. Using the ISM plugin, you can define policies that automatically handle index rollovers or deletions to fit your use case. For example, you can define a policy that moves your index into a read_only state after 30 days and then deletes it after a set period of 90 days. You can also set up the policy to send you a notification message when the index is deleted. You might want to perform an index rollover after a certain amount of time or run a force_merge operation on an index during off-peak hours to improve search performance during peak hours. To use the ISM plugin, your user role needs to be mapped to the all_access role that gives you full access to the cluster. To learn more, see Users and roles. ",
    "url": "http://localhost:4000/im-plugin/ism/index/",
    "relUrl": "/im-plugin/ism/index/"
  },"90": {
    "doc": "Index State Management",
    "title": "Get started with ISM",
    "content": "To get started, choose Index Management in OpenSearch Dashboards. Step 1: Set up policies . A policy is a set of rules that describes how an index should be managed. For information about creating a policy, see Policies. You can use the visual editor or JSON editor to create policies. Compared to the JSON editor, the visual editor offers a more structured way of defining policies by separating the process into creating error notifications, defining ISM templates, and adding states. We recommend using the visual editor if you want to see pre-defined fields, such as which actions you can assign to a state or under what conditions a state can transition into a destination state. Visual editor . | Choose the Index Policies tab. | Choose Create policy. | Choose Visual editor. | In the Policy info section, enter a policy ID and an optional description. | In the Error notification section, set up an optional error notification that gets sent whenever a policy execution fails. For more information, see Error notifications. If you’re using auto rollovers in your policy, we recommend setting up error notifications, which notify you of unexpectedly large indexes if rollovers fail. | In ISM templates, enter any ISM template patterns to automatically apply this policy to future indexes. For example, if you specify a template of sample-index*, the ISM plugin automatically applies this policy to any indexes whose names start with sample-index. Your pattern cannot contain any of the following characters: :, \", +, /, \\, |, ?, #, &gt;, and &lt;. | In States, add any states you want to include in the policy. Each state has actions the plugin executes when the index enters a certain state, and transitions, which have conditions that, when met, transition the index into a destination state. The first state you create in a policy is automatically set as the initial state. Each policy must have at least one state, but actions and transitions are optional. | Choose Create. | . JSON editor . | Choose the Index Policies tab. | Choose Create policy. | Choose JSON editor. | In the Name policy section, enter a policy ID. | In the Define policy section, enter your policy. | Choose Create. | . After you create a policy, your next step is to attach it to an index or indexes. You can set up an ism_template in the policy so when an index that matches the ISM template pattern is created, the plugin automatically attaches the policy to the index. The following example demonstrates how to create a policy that automatically gets attached to all indexes whose names start with index_name-. PUT _plugins/_ism/policies/policy_id { \"policy\": { \"description\": \"Example policy.\", \"default_state\": \"...\", \"states\": [...], \"ism_template\": { \"index_patterns\": [\"index_name-*\"], \"priority\": 100 } } } . If you have more than one template that matches an index pattern, ISM uses the priority value to determine which template to apply. For an example ISM template policy, see Sample policy with ISM template for auto rollover. Older versions of the plugin include the policy_id in an index template, so when an index is created that matches the index template pattern, the index will have the policy attached to it: . PUT _index_template/&lt;template_name&gt; { \"index_patterns\": [ \"index_name-*\" ], \"template\": { \"settings\": { \"opendistro.index_state_management.policy_id\": \"policy_id\" } } } . The opendistro.index_state_management.policy_id setting is deprecated. You can continue to automatically manage newly created indexes with the ISM template field. Step 2: Attach policies to indexes . | Choose indexes. | Choose the index or indexes that you want to attach your policy to. | Choose Apply policy. | From the Policy ID menu, choose the policy that you created. You can see a preview of your policy. | If your policy includes a rollover operation, specify a rollover alias. Make sure that the alias that you enter already exists. For more information about the rollover operation, see rollover. | Choose Apply. | . After you attach a policy to an index, ISM creates a job that runs every 5 minutes by default to perform policy actions, check conditions, and transition the index into different states. To change the default time interval for this job, see Settings. ISM does not run jobs if the cluster state is red. Step 3: Manage indexes . | Choose Managed indexes. | To change your policy, see Change Policy. | To attach a rollover alias to your index, select your policy and choose Add rollover alias. Make sure that the alias that you enter already exists. For more information about the rollover operation, see rollover. | To remove a policy, choose your policy, and then choose Remove policy. | To retry a policy, choose your policy, and then choose Retry policy. | . For information about managing your policies, see Managed indexes. ",
    "url": "http://localhost:4000/im-plugin/ism/index/#get-started-with-ism",
    "relUrl": "/im-plugin/ism/index/#get-started-with-ism"
  },"91": {
    "doc": "Managed Indices",
    "title": "Managed indices",
    "content": "You can change or update a policy using the managed index operations. This table lists the fields of managed index operations. | Parameter | Description | Type | Required | Read Only | . | name | The name of the managed index policy. | string | Yes | No | . | index | The name of the managed index that this policy is managing. | string | Yes | No | . | index_uuid | The uuid of the index. | string | Yes | No | . | enabled | When true, the managed index is scheduled and run by the scheduler. | boolean | Yes | No | . | enabled_time | The time the managed index was last enabled. If the managed index process is disabled, then this is null. | timestamp | Yes | Yes | . | last_updated_time | The time the managed index was last updated. | timestamp | Yes | Yes | . | schedule | The schedule of the managed index job. | object | Yes | No | . | policy_id | The name of the policy used by this managed index. | string | Yes | No | . | policy_seq_no | The sequence number of the policy used by this managed index. | number | Yes | No | . | policy_primary_term | The primary term of the policy used by this managed index. | number | Yes | No | . | policy_version | The version of the policy used by this managed index. | number | Yes | Yes | . | policy | The cached JSON of the policy for the policy_version that’s used during runs. If the policy is null, it means that this is the first execution of the job and the latest policy document is read in/saved. | object | No | No | . | change_policy | The information regarding what policy and state to change to. | object | No | No | . | policy_name | The name of the policy to update to. To update to the latest version, set this to be the same as the current policy_name. | string | No | Yes | . | state | The state of the managed index after it finishes updating. If no state is specified, it’s assumed that the policy structure did not change. | string | No | Yes | . The following example shows a managed index policy: . { \"managed_index\": { \"name\": \"my_index\", \"index\": \"my_index\", \"index_uuid\": \"sOKSOfkdsoSKeofjIS\", \"enabled\": true, \"enabled_time\": 1553112384, \"last_updated_time\": 1553112384, \"schedule\": { \"interval\": { \"period\": 1, \"unit\": \"MINUTES\", \"start_time\": 1553112384 } }, \"policy_id\": \"log_rotation\", \"policy_version\": 1, \"policy\": {...}, \"change_policy\": null } } . ",
    "url": "http://localhost:4000/im-plugin/ism/managedindexes/#managed-indices",
    "relUrl": "/im-plugin/ism/managedindexes/#managed-indices"
  },"92": {
    "doc": "Managed Indices",
    "title": "Change policy",
    "content": "You can change any managed index policy, but ISM has a few constraints in place to make sure that policy changes don’t break indices. If an index is stuck in its current state, never proceeding, and you want to update its policy immediately, make sure that the new policy includes the same state—same name, same actions, same order—as the old policy. In this case, even if the policy is in the middle of executing an action, ISM applies the new policy. If you update the policy without including an identical state, ISM updates the policy only after all actions in the current state finish executing. Alternately, you can choose a specific state in your old policy after which you want the new policy to take effect. To change a policy using OpenSearch Dashboards, do the following: . | Under Managed indices, choose the indices that you want to attach the new policy to. | To attach the new policy to indices in specific states, choose Choose state filters, and then choose those states. | Under Choose New Policy, choose the new policy. | To start the new policy for indices in the current state, choose Keep indices in their current state after the policy takes effect. | To start the new policy in a specific state, choose Start from a chosen state after changing policies, and then choose the default start state in your new policy. | . ",
    "url": "http://localhost:4000/im-plugin/ism/managedindexes/#change-policy",
    "relUrl": "/im-plugin/ism/managedindexes/#change-policy"
  },"93": {
    "doc": "Managed Indices",
    "title": "Managed Indices",
    "content": " ",
    "url": "http://localhost:4000/im-plugin/ism/managedindexes/",
    "relUrl": "/im-plugin/ism/managedindexes/"
  },"94": {
    "doc": "Policies",
    "title": "Policies",
    "content": "Policies are JSON documents that define the following: . | The states that an index can be in, including the default state for new indexes. For example, you might name your states “hot,” “warm,” “delete,” and so on. For more information, see States. | Any actions that you want the plugin to take when an index enters a state, such as performing a rollover. For more information, see Actions. | The conditions that must be met for an index to move into a new state, known as transitions. For example, if an index is more than eight weeks old, you might want to move it to the “delete” state. For more information, see Transitions. | . In other words, a policy defines the states that an index can be in, the actions to perform when in a state, and the conditions that must be met to transition between states. You have complete flexibility in the way you can design your policies. You can create any state, transition to any other state, and specify any number of actions in each state. This table lists the relevant fields of a policy. | Field | Description | Type | Required | Read Only | . | policy_id | The name of the policy. | string | Yes | Yes | . | description | A human-readable description of the policy. | string | Yes | No | . | ism_template | Specify an ISM template pattern that matches the index to apply the policy. | nested list of objects | No | No | . | last_updated_time | The time the policy was last updated. | timestamp | Yes | Yes | . | error_notification | The destination and message template for error notifications. The destination could be Amazon Chime, Slack, or a webhook URL. | object | No | No | . | default_state | The default starting state for each index that uses this policy. | string | Yes | No | . | states | The states that you define in the policy. | nested list of objects | Yes | No | . | States | Actions | ISM supported operations . | force_merge | read_only | read_write | replica_count | shrink | close | open | delete | rollover | notification | snapshot | index_priority | allocation | rollup | . | Transitions | Error notifications | Sample policy with ISM template for auto rollover | Example policy with ISM templates for the alias action | Example policy | . ",
    "url": "http://localhost:4000/im-plugin/ism/policies/",
    "relUrl": "/im-plugin/ism/policies/"
  },"95": {
    "doc": "Policies",
    "title": "States",
    "content": "A state is the description of the status that the managed index is currently in. A managed index can be in only one state at a time. Each state has associated actions that are executed sequentially on entering a state and transitions that are checked after all the actions have been completed. This table lists the parameters that you can define for a state. | Field | Description | Type | Required | . | name | The name of the state. | string | Yes | . | actions | The actions to execute after entering a state. For more information, see Actions. | nested list of objects | Yes | . | transitions | The next states and the conditions required to transition to those states. If no transitions exist, the policy assumes that it’s complete and can now stop managing the index. For more information, see Transitions. | nested list of objects | Yes | . ",
    "url": "http://localhost:4000/im-plugin/ism/policies/#states",
    "relUrl": "/im-plugin/ism/policies/#states"
  },"96": {
    "doc": "Policies",
    "title": "Actions",
    "content": "Actions are the steps that the policy sequentially executes on entering a specific state. ISM executes actions in the order in which they are defined. For example, if you define actions [A,B,C,D], ISM executes action A, and then goes into a sleep period based on the cluster setting plugins.index_state_management.job_interval. Once the sleep period ends, ISM continues to execute the remaining actions. However, if ISM cannot successfully execute action A, the operation ends, and actions B, C, and D do not get executed. Optionally, you can define an action’s timeout period, which, if exceeded, forcibly fails the action. For example, if timeout is set to 1d, and ISM has not completed the action within one day, even after retries, the action fails. This table lists the parameters that you can define for an action. | Parameter | Description | Type | Required | Default | . | timeout | The timeout period for the action. Accepts time units for minutes, hours, and days. | time unit | No | - | . | retry | The retry configuration for the action. | object | No | Specific to action | . The retry operation has the following parameters: . | Parameter | Description | Type | Required | Default | . | count | The number of retry counts. | number | Yes | - | . | backoff | The backoff policy type to use when retrying. Valid values are Exponential, Constant, and Linear. | string | No | Exponential | . | delay | The time to wait between retries. Accepts time units for minutes, hours, and days. | time unit | No | 1 minute | . The following example action has a timeout period of one hour. The policy retries this action three times with an exponential backoff policy, with a delay of 10 minutes between each retry: . \"actions\": { \"timeout\": \"1h\", \"retry\": { \"count\": 3, \"backoff\": \"exponential\", \"delay\": \"10m\" } } . For a list of available unit types, see Supported units. ",
    "url": "http://localhost:4000/im-plugin/ism/policies/#actions",
    "relUrl": "/im-plugin/ism/policies/#actions"
  },"97": {
    "doc": "Policies",
    "title": "ISM supported operations",
    "content": "ISM supports the following operations: . | force_merge | read_only | read_write | replica_count | shrink | close | open | delete | rollover | notification | snapshot | index_priority | allocation | rollup | . force_merge . Reduces the number of Lucene segments by merging the segments of individual shards. This operation attempts to set the index to a read-only state before starting the merging process. | Parameter | Description | Type | Required | . | max_num_segments | The number of segments to reduce the shard to. | number | Yes | . { \"force_merge\": { \"max_num_segments\": 1 } } . read_only . Sets a managed index to be read only. { \"read_only\": {} } . read_write . Sets a managed index to be writeable. { \"read_write\": {} } . replica_count . Sets the number of replicas to assign to an index. | Parameter | Description | Type | Required | . | number_of_replicas | Defines the number of replicas to assign to an index. | number | Yes | . { \"replica_count\": { \"number_of_replicas\": 2 } } . For information about setting replicas, see Primary and replica shards. shrink . Allows you to reduce the number of primary shards in your indexes. With this action, you can specify: . | The number of primary shards that the target index should contain. | A max shard size for the primary shards in the target index. | Specify a percentage to shrink the number of primary shards in the target index. | . \"shrink\": { \"num_new_shards\": 1, \"target_index_name_template\": { \"source\": \"_shrunken\" }, \"aliases\": [ { \"my-alias\": {} } ], \"force_unsafe\": false } . | Parameter | Description | Type | Example | Required | . | num_new_shards | The maximum number of primary shards in the shrunken index. | integer | 5 | Yes, however it cannot be used with max_shard_size or percentage_of_source_shards | . | max_shard_size | The maximum size in bytes of a shard for the target index. | keyword | 5gb | Yes, however it cannot be used with num_new_shards or percentage_of_source_shards | . | percentage_of_source_shards | Percentage of the number of original primary shards to shrink. This parameter indicates the minimum percentage to use when shrinking the number of primary shards. Must be between 0.0 and 1.0, exclusive. | Percentage | 0.5 | Yes, however it cannot be used with max_shard_size or num_new_shards | . | target_index_name_template | The name of the shrunken index. Accepts strings and the Mustache variables and. | string or Mustache template | {\"source\": \"_shrunken\"} | No | . | aliases | Aliases to add to the new index. | object | myalias | No, but must be an array of alias objects | . | force_unsafe | If true, executes the shrink action even if there are no replicas. | boolean | false | No | . If you want to add aliases to the action, the parameter must include an array of alias objects. For example, . \"aliases\": [ { \"my-alias\": {} }, { \"my-second-alias\": { \"is_write_index\": false, \"filter\": { \"multi_match\": { \"query\": \"QUEEN\", \"fields\": [\"speaker\", \"text_entry\"] } }, \"index_routing\" : \"1\", \"search_routing\" : \"1\" } }, ] . close . Closes the managed index. { \"close\": {} } . Closed indexes remain on disk, but consume no CPU or memory. You can’t read from, write to, or search closed indexes. Closing an index is a good option if you need to retain data for longer than you need to actively search it and have sufficient disk space on your data nodes. If you need to search the data again, reopening a closed index is simpler than restoring an index from a snapshot. open . Opens a managed index. { \"open\": {} } . delete . Deletes a managed index. { \"delete\": {} } . rollover . Rolls an alias over to a new index when the managed index meets one of the rollover conditions. Important: ISM checks the conditions for operations on every execution of the policy based on the set interval, not continuously. The rollover will be performed if the value has reached or exceeded the configured limit when the check is performed. For example with min_size configured to a value of 100GiB, ISM might check the index at 99 GiB and not perform the rollover. However, if the index has grown past the limit (e.g. 105GiB) by the next check, the operation is performed. The index format must match the pattern: ^.*-\\d+$. For example, (logs-000001). Set index.plugins.index_state_management.rollover_alias as the alias to rollover. | Parameter | Description | Type | Example | Required | . | min_size | The minimum size of the total primary shard storage (not counting replicas) required to roll over the index. For example, if you set min_size to 100 GiB and your index has 5 primary shards and 5 replica shards of 20 GiB each, the total size of all primary shards is 100 GiB, so the rollover occurs. See Important note above. | string | 20gb or 5mb | No | . | min_primary_shard_size | The minimum storage size of a single primary shard required to roll over the index. For example, if you set min_primary_shard_size to 30 GiB and one of the primary shards in the index has a size greater than the condition, the rollover occurs. See Important note above. | string | 20gb or 5mb | No | . | min_doc_count | The minimum number of documents required to roll over the index. See Important note above. | number | 2000000 | No | . | min_index_age | The minimum age required to roll over the index. Index age is the time between its creation and the present. Supported units are d (days), h (hours), m (minutes), s (seconds), ms (milliseconds), and micros (microseconds). See Important note above. | string | 5d or 7h | No | . { \"rollover\": { \"min_size\": \"50gb\" } } . { \"rollover\": { \"min_primary_shard_size\": \"30gb\" } } . { \"rollover\": { \"min_doc_count\": 100000000 } } . { \"rollover\": { \"min_index_age\": \"30d\" } } . notification . Sends you a notification. | Parameter | Description | Type | Required | . | destination | The destination URL. | Slack, Amazon Chime, or webhook URL | Yes | . | message_template | The text of the message. You can add variables to your messages using Mustache templates. | object | Yes | . The destination system must return a response otherwise the notification operation throws an error. Example 1: Chime notification . { \"notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;url&gt;\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } . Example 2: Custom webhook notification . { \"notification\": { \"destination\": { \"custom_webhook\": { \"url\": \"https://&lt;your_webhook&gt;\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } . Example 3: Slack notification . { \"notification\": { \"destination\": { \"slack\": { \"url\": \"https://hooks.slack.com/services/xxx/xxxxxx\" } }, \"message_template\": { \"source\": \"the index is {{ctx.index}}\" } } } . You can use ctx variables in your message to represent a number of policy parameters based on the past executions of your policy. For example, if your policy has a rollover action, you can use {{ctx.action.name}} in your message to represent the name of the rollover. The following ctx variable options are available for every policy: . Guaranteed variables . | Parameter | Description | Type | . | index | The name of the index. | string | . | index_uuid | The uuid of the index. | string | . | policy_id | The name of the policy. | string | . snapshot . Back up your cluster’s indexes and state. For more information about snapshots, see Take and restore snapshots. The snapshot operation has the following parameters: . | Parameter | Description | Type | Required | Default | . | repository | The repository name that you register through the native snapshot API operations. | string | Yes | - | . | snapshot | The name of the snapshot. Accepts strings and the Mustache variables and. If the Mustache variables are invalid, then the snapshot name defaults to the index’s name. | string or Mustache template | Yes | - | . { \"snapshot\": { \"repository\": \"my_backup\", \"snapshot\": \"\" } } . index_priority . Set the priority for the index in a specific state. Unallocated shards of indexes are recovered in the order of their priority, whenever possible. The indexes with higher priority values are recovered first followed by the indexes with lower priority values. The index_priority operation has the following parameter: . | Parameter | Description | Type | Required | Default | . | priority | The priority for the index as soon as it enters a state. | number | Yes | 1 | . \"actions\": [ { \"index_priority\": { \"priority\": 50 } } ] . allocation . Allocate the index to a node with a specific attribute set like this. For example, setting require to warm moves your data only to “warm” nodes. The allocation operation has the following parameters: . | Parameter | Description | Type | Required | . | require | Allocate the index to a node with a specified attribute. | string | Yes | . | include | Allocate the index to a node with any of the specified attributes. | string | Yes | . | exclude | Don’t allocate the index to a node with any of the specified attributes. | string | Yes | . | wait_for | Wait for the policy to execute before allocating the index to a node with a specified attribute. | string | Yes | . \"actions\": [ { \"allocation\": { \"require\": { \"temp\": \"warm\" } } } ] . rollup . Index rollup lets you periodically reduce data granularity by rolling up old data into summarized indexes. Rollup jobs can be continuous or non-continuous. A rollup job created using an ISM policy can only be non-continuous. Path and HTTP methods . PUT _plugins/_rollup/jobs/&lt;rollup_id&gt; GET _plugins/_rollup/jobs/&lt;rollup_id&gt; DELETE _plugins/_rollup/jobs/&lt;rollup_id&gt; POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_start POST _plugins/_rollup/jobs/&lt;rollup_id&gt;/_stop GET _plugins/_rollup/jobs/&lt;rollup_id&gt;/_explain . Sample ISM rollup policy . { \"policy\": { \"description\": \"Sample rollup\" , \"default_state\": \"rollup\", \"states\": [ { \"name\": \"rollup\", \"actions\": [ { \"rollup\": { \"ism_rollup\": { \"description\": \"Creating rollup through ISM\", \"target_index\": \"target\", \"page_size\": 1000, \"dimensions\": [ { \"date_histogram\": { \"fixed_interval\": \"60m\", \"source_field\": \"order_date\", \"target_field\": \"order_date\", \"timezone\": \"America/Los_Angeles\" } }, { \"terms\": { \"source_field\": \"customer_gender\", \"target_field\": \"customer_gender\" } }, { \"terms\": { \"source_field\": \"day_of_week\", \"target_field\": \"day_of_week\" } } ], \"metrics\": [ { \"source_field\": \"taxless_total_price\", \"metrics\": [ { \"sum\": {} } ] }, { \"source_field\": \"total_quantity\", \"metrics\": [ { \"avg\": {} }, { \"max\": {} } ] } ] } } } ], \"transitions\": [] } ] } } . Request fields . Request fields are required when creating an ISM policy. You can reference the Index rollups API page for request field options. Adding a rollup policy in Dashboards . To add a rollup policy in Dashboards, follow the steps below. | Select the menu button on the top-left of the Dashboards user interface. | In the Dashboards menu, select Index Management. | On the next screen select Rollup jobs. | Select the Create rollup button. | Follow the steps in the Create rollup job wizard. | Add a name for the policy in the Name box. | You can reference the Index rollups API page to configure the rollup policy. | Finally, select the Create button on the bottom-right of the Dashboards user interface. | . ",
    "url": "http://localhost:4000/im-plugin/ism/policies/#ism-supported-operations",
    "relUrl": "/im-plugin/ism/policies/#ism-supported-operations"
  },"98": {
    "doc": "Policies",
    "title": "Transitions",
    "content": "Transitions define the conditions that need to be met for a state to change. After all actions in the current state are completed, the policy starts checking the conditions for transitions. ISM evaluates transitions in the order in which they are defined. For example, if you define transitions: [A,B,C,D], ISM iterates through this list of transitions until it finds a transition that evaluates to true, it then stops and sets the next state to the one defined in that transition. On its next execution, ISM dismisses the rest of the transitions and starts in that new state. If you don’t specify any conditions in a transition and leave it empty, then it’s assumed to be the equivalent of always true. This means that the policy transitions the index to this state the moment it checks. This table lists the parameters you can define for transitions. | Parameter | Description | Type | Required | . | state_name | The name of the state to transition to if the conditions are met. | string | Yes | . | conditions | List the conditions for the transition. | list | Yes | . The conditions object has the following parameters: . | Parameter | Description | Type | Required | . | min_index_age | The minimum age of the index required to transition. | string | No | . | min_rollover_age | The minimum age required after a rollover has occurred to transition to the next state. | string | No | . | min_doc_count | The minimum document count of the index required to transition. | number | No | . | min_size | The minimum size of the total primary shard storage (not counting replicas) required to transition. For example, if you set min_size to 100 GiB and your index has 5 primary shards and 5 replica shards of 20 GiB each, the total size of all primary shards is 100 GiB, so your index is transitioned to the next state. | string | No | . | cron | The cron job that triggers the transition if no other transition happens first. | object | No | . | cron.cron.expression | The cron expression that triggers the transition. | string | Yes | . | cron.cron.timezone | The timezone that triggers the transition. | string | Yes | . The following example transitions the index to a cold state after a period of 30 days: . \"transitions\": [ { \"state_name\": \"cold\", \"conditions\": { \"min_index_age\": \"30d\" } } ] . ISM checks the conditions on every execution of the policy based on the set interval. This example uses the cron condition to transition indexes every Saturday at 5:00 PT: . \"transitions\": [ { \"state_name\": \"cold\", \"conditions\": { \"cron\": { \"cron\": { \"expression\": \"* 17 * * SAT\", \"timezone\": \"America/Los_Angeles\" } } } } ] . Note that this condition does not execute at exactly 5:00 PM; the job still executes based off the job_interval setting. Due to this variance in start time and the amount of time that it can take for actions to complete prior to checking transition conditions, we recommend against overly narrow cron expressions. For example, don’t use 15 17 * * SAT (5:15 PM on Saturday). A window of an hour, which this example uses, is generally sufficient, but you might increase it to 2–3 hours to avoid missing the window and having to wait a week for the transition to occur. Alternately, you could use a broader expression such as * * * * SAT,SUN to have the transition occur at any time during the weekend. For information on writing cron expressions, see Cron expression reference. ",
    "url": "http://localhost:4000/im-plugin/ism/policies/#transitions",
    "relUrl": "/im-plugin/ism/policies/#transitions"
  },"99": {
    "doc": "Policies",
    "title": "Error notifications",
    "content": "The error_notification operation sends you a notification if your managed index fails. It notifies a single destination or notification channel with a custom message. Set up error notifications at the policy level: . { \"policy\": { \"description\": \"hot warm delete workflow\", \"default_state\": \"hot\", \"schema_version\": 1, \"error_notification\": { }, \"states\": [ ] } } . | Parameter | Description | Type | Required | . | destination | The destination URL. | Slack, Amazon Chime, or webhook URL | Yes if channel isn’t specified | . | channel | A notification channel’s ID | string | Yes if destination isn’t specified | . | message_template | The text of the message. You can add variables to your messages using Mustache templates. | object | Yes | . The destination system must return a response otherwise the error_notification operation throws an error. Example 1: Chime notification . { \"error_notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;url&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . Example 2: Custom webhook notification . { \"error_notification\": { \"destination\": { \"custom_webhook\": { \"url\": \"https://&lt;your_webhook&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . Example 3: Slack notification . { \"error_notification\": { \"destination\": { \"slack\": { \"url\": \"https://hooks.slack.com/services/xxx/xxxxxx\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . Example 4: Using a notification channel . { \"error_notification\": { \"channel\": { \"id\": \"some-channel-config-id\" }, \"message_template\": { \"source\": \"The index {{ctx.index}} failed during policy execution.\" } } } . You can use the same options for ctx variables as the notification operation. ",
    "url": "http://localhost:4000/im-plugin/ism/policies/#error-notifications",
    "relUrl": "/im-plugin/ism/policies/#error-notifications"
  },"100": {
    "doc": "Policies",
    "title": "Sample policy with ISM template for auto rollover",
    "content": "The following sample template policy is for a rollover use case. If you want to skip rollovers for an index, set index.plugins.index_state_management.rollover_skip to true in the settings of that index. | Create a policy with an ism_template field: . PUT _plugins/_ism/policies/rollover_policy { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } } ], \"transitions\": [] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . You need to specify the index_patterns field. If you don’t specify a value for priority, it defaults to 0. | Set up a template with the rollover_alias as log : . PUT _index_template/ism_rollover { \"index_patterns\": [\"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } . | Create an index with the log alias: . PUT log-000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } . | Index a document to trigger the rollover condition: . POST log/_doc { \"message\": \"dummy\" } . | Verify if the policy is attached to the log-000001 index: . GET _plugins/_ism/explain/log-000001?pretty . | . ",
    "url": "http://localhost:4000/im-plugin/ism/policies/#sample-policy-with-ism-template-for-auto-rollover",
    "relUrl": "/im-plugin/ism/policies/#sample-policy-with-ism-template-for-auto-rollover"
  },"101": {
    "doc": "Policies",
    "title": "Example policy with ISM templates for the alias action",
    "content": "The following example policy is for an alias action use case. In the following example, the first job will trigger the rollover action, and a new index will be created. Next, another document is added to the two indexes. The new job will then cause the second index to point to the log alias, and the older index will be removed due to the alias action. First, create an ISM policy: . PUT /_plugins/_ism/policies/rollover_policy?pretty { \"policy\": { \"description\": \"Example rollover policy.\", \"default_state\": \"rollover\", \"states\": [ { \"name\": \"rollover\", \"actions\": [ { \"rollover\": { \"min_doc_count\": 1 } } ], \"transitions\": [{ \"state_name\": \"alias\", \"conditions\": { \"min_doc_count\": \"2\" } }] }, { \"name\": \"alias\", \"actions\": [ { \"alias\": { \"actions\": [ { \"remove\": { \"alias\": \"log\" } } ] } } ] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . Next, create an index template on which to enable the policy: . PUT /_index_template/ism_rollover? { \"index_patterns\": [\"log*\"], \"template\": { \"settings\": { \"plugins.index_state_management.rollover_alias\": \"log\" } } } . copy . Next, change the cluster settings to trigger jobs every minute: . PUT /_cluster/settings?pretty=true { \"persistent\" : { \"plugins.index_state_management.job_interval\" : 1 } } . copy . Next, create a new index: . PUT /log-000001 { \"aliases\": { \"log\": { \"is_write_index\": true } } } . copy . Finally, add a document to the index to trigger the job: . POST /log-000001/_doc { \"message\": \"dummy\" } . copy . You can verify these steps using the Alias and Index API: . GET /_cat/indices?pretty . copy . GET /_cat/aliases?pretty . copy . Note: The index and remove_index parameters are not allowed with alias action policies. Only the add and remove alias action parameters are allowed. ",
    "url": "http://localhost:4000/im-plugin/ism/policies/#example-policy-with-ism-templates-for-the-alias-action",
    "relUrl": "/im-plugin/ism/policies/#example-policy-with-ism-templates-for-the-alias-action"
  },"102": {
    "doc": "Policies",
    "title": "Example policy",
    "content": "The following example policy implements a hot, warm, and delete workflow. You can use this policy as a template to prioritize resources to your indexes based on their levels of activity. In this case, an index is initially in a hot state. After a day, it changes to a warm state, where the number of replicas increases to 5 to improve the read performance. After 30 days, the policy moves this index into a delete state. The service sends a notification to a Chime room that the index is being deleted, and then permanently deletes it. { \"policy\": { \"description\": \"hot warm delete workflow\", \"default_state\": \"hot\", \"schema_version\": 1, \"states\": [ { \"name\": \"hot\", \"actions\": [ { \"rollover\": { \"min_index_age\": \"1d\", \"min_primary_shard_size\": \"30gb\" } } ], \"transitions\": [ { \"state_name\": \"warm\" } ] }, { \"name\": \"warm\", \"actions\": [ { \"replica_count\": { \"number_of_replicas\": 5 } } ], \"transitions\": [ { \"state_name\": \"delete\", \"conditions\": { \"min_index_age\": \"30d\" } } ] }, { \"name\": \"delete\", \"actions\": [ { \"notification\": { \"destination\": { \"chime\": { \"url\": \"&lt;URL&gt;\" } }, \"message_template\": { \"source\": \"The index {{ctx.index}} is being deleted\" } } }, { \"delete\": {} } ] } ], \"ism_template\": { \"index_patterns\": [\"log*\"], \"priority\": 100 } } } . This diagram shows the states, transitions, and actions of the above policy as a finite-state machine. For more information about finite-state machines, see Wikipedia. ",
    "url": "http://localhost:4000/im-plugin/ism/policies/#example-policy",
    "relUrl": "/im-plugin/ism/policies/#example-policy"
  },"103": {
    "doc": "Settings",
    "title": "ISM settings",
    "content": "We don’t recommend changing these settings; the defaults should work well for most use cases. Index State Management (ISM) stores its configuration in the .opendistro-ism-config index. Don’t modify this index without using the ISM API operations. All settings are available using the OpenSearch _cluster/settings operation. None require a restart, and all can be marked persistent or transient. | Setting | Default | Description | . | plugins.index_state_management.enabled | True | Specifies whether ISM is enabled or not. | . | plugins.index_state_management.job_interval | 5 minutes | The interval at which the managed index jobs are run. | . | plugins.index_state_management.jitter | 0.6 | A randomized delay that is added to a job’s base run time to prevent a surge of activity from all indices at the same time. A value of 0.6 means a delay of 0-60% of a job interval is added to the base interval. For example, if you have a base interval time of 30 minutes, a value of 0.6 means an amount anywhere between 0 to 18 minutes gets added to your job interval. Maximum is 1, which means an additional interval time of 100%. This maximum cannot exceed plugins.jobscheduler.jitter_limit, which also has a default of 0.6. For example, if plugins.index_state_management.jitter is set to 0.8, ISM uses plugins.jobscheduler.jitter_limit of 0.6 instead. | . | plugins.index_state_management.coordinator.sweep_period | 10 minutes | How often the routine background sweep is run. | . | plugins.index_state_management.coordinator.backoff_millis | 50 milliseconds | The backoff time between retries for failures in the ManagedIndexCoordinator (such as when we update managed indices). | . | plugins.index_state_management.coordinator.backoff_count | 2 | The count of retries for failures in the ManagedIndexCoordinator. | . | plugins.index_state_management.history.enabled | True | Specifies whether audit history is enabled or not. The logs from ISM are automatically indexed to a logs document. | . | plugins.index_state_management.history.max_docs | 2,500,000 | The maximum number of documents before rolling over the audit history index. | . | plugins.index_state_management.history.max_age | 24 hours | The maximum age before rolling over the audit history index. | . | plugins.index_state_management.history.rollover_check_period | 8 hours | The time between rollover checks for the audit history index. | . | plugins.index_state_management.history.rollover_retention_period | 30 days | How long audit history indices are kept. | . | plugins.index_state_management.allow_list | All actions | List of actions that you can use. | . ",
    "url": "http://localhost:4000/im-plugin/ism/settings/#ism-settings",
    "relUrl": "/im-plugin/ism/settings/#ism-settings"
  },"104": {
    "doc": "Settings",
    "title": "Settings",
    "content": " ",
    "url": "http://localhost:4000/im-plugin/ism/settings/",
    "relUrl": "/im-plugin/ism/settings/"
  },"105": {
    "doc": "Refresh search analyzer",
    "title": "Refresh search analyzer",
    "content": "With ISM installed, you can refresh search analyzers in real time with the following API: . POST /_plugins/_refresh_search_analyzers/&lt;index or alias or wildcard&gt; . For example, if you change the synonym list in your analyzer, the change takes effect without you needing to close and reopen the index. To work, the token filter must have an updateable flag of true: . { \"analyzer\": { \"my_synonyms\": { \"tokenizer\": \"whitespace\", \"filter\": [ \"synonym\" ] } }, \"filter\": { \"synonym\": { \"type\": \"synonym_graph\", \"synonyms_path\": \"synonyms.txt\", \"updateable\": true } } } . ",
    "url": "http://localhost:4000/im-plugin/refresh-analyzer/index/",
    "relUrl": "/im-plugin/refresh-analyzer/index/"
  },"106": {
    "doc": "Index management security",
    "title": "Index management security",
    "content": "Using the security plugin with index management lets you limit non-admin users to certain actions. For example, you might want to set up your security such that a group of users can only read ISM policies, while others can create, delete, or change policies. All index management data are protected as system indices, and only a super admin or an admin with a Transport Layer Security (TLS) certificate can access system indices. For more information, see System indices. ",
    "url": "http://localhost:4000/im-plugin/security/",
    "relUrl": "/im-plugin/security/"
  },"107": {
    "doc": "Index management security",
    "title": "Basic permissions",
    "content": "The security plugin comes with one role that offers full access to index management: index_management_full_access. For a description of the role’s permissions, see Predefined roles. With security enabled, users not only need the correct index management permissions, but they also need permissions to execute actions to involved indices. For example, if a user wants to use the REST API to attach a policy that executes a rollup job to an index named system-logs, they would need the permissions to attach a policy and execute a rollup job, as well as access to system-logs. Finally, with the exceptions of Create Policy, Get Policy, and Delete Policy, users also need the indices:admin/opensearch/ism/managedindex permission to execute ISM APIs. ",
    "url": "http://localhost:4000/im-plugin/security/#basic-permissions",
    "relUrl": "/im-plugin/security/#basic-permissions"
  },"108": {
    "doc": "Index management security",
    "title": "(Advanced) Limit access by backend role",
    "content": "You can use backend roles to configure fine-grained access to index management policies and actions. For example, users of different departments in an organization might view different policies depending on what roles and permissions they are assigned. First, ensure your users have the appropriate backend roles. Backend roles usually come from an LDAP server or SAML provider. However, if you use the internal user database, you can use the REST API to add them manually. Use the REST API to enable the following setting: . PUT _cluster/settings { \"transient\": { \"plugins.index_management.filter_by_backend_roles\": \"true\" } } . With security enabled, only users who share at least one backend role can see and execute the policies and actions relevant to their roles. For example, consider a scenario with three users: John and Jill, who have the backend role helpdesk_staff, and Jane, who has the backend role phone_operator. John wants to create a policy that performs a rollup job on an index named airline_data, so John would need a backend role that has permissions to access that index, create relevant policies, and execute relevant actions, and Jill would be able to access the same index, policy, and job. However, Jane cannot access or edit those resources or actions. ",
    "url": "http://localhost:4000/im-plugin/security/#advanced-limit-access-by-backend-role",
    "relUrl": "/im-plugin/security/#advanced-limit-access-by-backend-role"
  },"109": {
    "doc": "Advanced Analytics",
    "title": "Advanced Analytics",
    "content": "Advanced Analytics . ",
    "url": "http://localhost:4000/documentation/adv-analytics/index/",
    "relUrl": "/documentation/adv-analytics/index/"
  },"110": {
    "doc": "Advanced Analytics",
    "title": "Introduction",
    "content": " ",
    "url": "http://localhost:4000/documentation/adv-analytics/index/#introduction",
    "relUrl": "/documentation/adv-analytics/index/#introduction"
  },"111": {
    "doc": "Alerting",
    "title": "Alerting",
    "content": " ",
    "url": "http://localhost:4000/documentation/alerting/index/",
    "relUrl": "/documentation/alerting/index/"
  },"112": {
    "doc": "Analytics",
    "title": "Analytics",
    "content": " ",
    "url": "http://localhost:4000/documentation/analytics/index/",
    "relUrl": "/documentation/analytics/index/"
  },"113": {
    "doc": "Dashboards",
    "title": "Dashboards",
    "content": " ",
    "url": "http://localhost:4000/documentation/dashboards/index/",
    "relUrl": "/documentation/dashboards/index/"
  },"114": {
    "doc": "Data Collection",
    "title": "Data Collection",
    "content": " ",
    "url": "http://localhost:4000/documentation/data-collection/index/",
    "relUrl": "/documentation/data-collection/index/"
  },"115": {
    "doc": "Data Defination",
    "title": "Data Defination",
    "content": " ",
    "url": "http://localhost:4000/documentation/data-defination/index/",
    "relUrl": "/documentation/data-defination/index/"
  },"116": {
    "doc": "About Documentation",
    "title": "About Documentation",
    "content": "Elysium Documentation . About DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout DocumentationAbout Documentation . ",
    "url": "http://localhost:4000/documentation/index/",
    "relUrl": "/documentation/index/"
  },"117": {
    "doc": "Elysium Full Text Search",
    "title": "Elysium Full Text Search",
    "content": "search engine . ",
    "url": "http://localhost:4000/documentation/search/index/",
    "relUrl": "/documentation/search/index/"
  },"118": {
    "doc": "Elysium Full Text Search",
    "title": "Introduction",
    "content": "Elysium Search is a search tool designed to help users quickly find and filter data within a database. It allows users to search for specific values or patterns within a dataset, and offers a range of options for customizing the search process. ",
    "url": "http://localhost:4000/documentation/search/index/#introduction",
    "relUrl": "/documentation/search/index/#introduction"
  },"119": {
    "doc": "Elysium Full Text Search",
    "title": "How to Guides",
    "content": "Add Index To add an index to Elysium Search, follow these steps: . | Open the Elysium Search. | Click on the three horizontal bars to open side menu. | Click on the “Stack Management“ button. | Click on the “Index Patterns” button. | Click on the “Create Index Pattern” button. | Search for the table name | Click on the “Next Step“ button and wait until the load completes (at the right top corner). | Once loaded use “Time field“ dropdown to select the timestamp. | Selecting timestamp is mandatory to preform search. Do not create index without selecting the timestamp. | Click on the “Create Index pattern” button. | . Set Result Limit for a search in Elysium Search, follow these steps: . | Open the Elysium Search. | Click on the three horizontal bars to open side menu. | Click on the “Stack Management“ button. | Click on the “Advanced Settings” button. | Find “Number of rows” and change to desired number. | Click on the “Save changes” to save the setting. | . Elysium Search allows users to specify the time zone for their searches. To set the time zone, follow these steps: . | Open the Elysium SaaS. | Click on the “user icon” at the top right corner and then click “My account“. | Click “Timezone” for the dropdown and select to the desired time zone. | Once selected then Click on the button “Save“ which requires password. | Enter the password and click on the button “YES“. | . How to search: . | Open the Elysium Search. | Click on the three horizontal bars to open side menu. | Click on the “Discover“ button. | Click on the “index“ dropdown to select the index on which search needs to performed. | Select the time range from right top corner. | Use search bar for entering different types of searches. | Click on the “Refresh“/”Update” button for results. | . ",
    "url": "http://localhost:4000/documentation/search/index/#how-to-guides",
    "relUrl": "/documentation/search/index/#how-to-guides"
  },"120": {
    "doc": "Elysium Full Text Search",
    "title": "Different types of searches you can use to filter data.",
    "content": "Unquoted Search: . Use unquoted search to filter data that partially matches with the value. For example, to filter data where the username field values are like “john white”, “john whitehead”,”sara white”, etc., use the following syntax: . username: white . The field parameter is optional. If not provided, all fields will be searched for the given value. For example, to search all fields for “white”, use the following: . white . Quoted Search: . Use quoted search to filter data that exactly matches with the value. For example, to filter data where the username field value is “john white”, use the following syntax: . username: \"john white\" . The field parameter is optional. If not provided, Smart Search will be enabled and it will identify fields that might be a possible match and search those fields for the given value. For more information on Smart Search, refer Smart Search. To use Exact Match, use the following: . \"john white\" . Wildcard Search: . Even though unquoted search is used for partial matches, there are cases where wildcard search can be useful. To filter data that starts with “tom”, use the following syntax: . username: tom* . To filter data that ends with “sam”, use the following syntax: . username: *sam . Wildcard search can only be used with unquoted searches. If the wildcard () is used in a quoted search, it will be considered as a literal. For example, “sam” will search for “sam*”. Ulike unquoted search which comes with default wildcards at the start and end of the input value. For example, sam will be queried as sam. Meaning any characters can be before and after “sam“. So, when using wildcards explicitly, it removes the default wildcards and treats the input (*) as a wildcard. Wildcards can also be used to find fields where a value exists. Use the following syntax to get all values without nulls: . username: * . Note: Wildcard Searches doesn’t query on flatten columns like raw.src.ip while column is not mentioned. Matching Multiple Fields: . Wildcards can also be used to query multiple fields. For example, to search for documents where any sub-field of “http.response” equals “error”, use the following: . http.response.*: \"error\" . Negating a Query: . To negate or exclude a set of data, use the “not” keyword. For example, to filter documents where the “http.request.method” is not “GET”, use the following query: . NOT http.request.method: “GET” . Negating also contains the null value while a specific search will always negate null values. Combining Multiple Queries: . To combine multiple queries, use the “and” or “or” keywords. For example, to find documents where the “http.request.method” is “GET” or the “http.response.status_code” is 400, use the following query: . http.request.method: “GET” OR http.response.status_code: “400” . Similarly, to find documents where the “http.request.method” is “GET” and the “http.response.status_code” is 400, use this query: . http.request.method: “GET” AND http.response.status_code: “400” . To specify precedence when combining multiple queries, use parentheses. For example, to find documents where the “http.request.method” is “GET” and the “http.response.status_code” is 200, or the “http.request.method is POST” and “http.response.status_code” is 400, use the following: . (http.request.method: “GET” AND http.response.status_code: “200”) OR (http.request.method: “POST” AND http.response.status_code: “400”) . You can also use parentheses for shorthand syntax when querying multiple values for the same field. For example, to find documents where the “http.request.method” is “GET”, “POST”, or “DELETE”, use the following: . http.request.method: (“GET” OR “POST” OR “DELETE”) . http.request.method:”GET” OR “POST“ This would return results that either contain “GET “in the specified field http.request.method or contain the term “POST” anywhere in the document. Precedence: When using multiple logical operators in a search, it is important to consider the precedence of the operators. The AND operator has the highest precedence, followed by OR. So the search term “A AND B OR C” will be evaluated as “(A AND B) OR C”, while the search term “A OR B AND C” will be evaluated as “A OR (B AND C)”. Escaping Special Characters: . There may be times when you need to search for a value that includes special characters. In these cases, you will need to escape these characters. To search for documents where  http.request.referrer  is  https://example.com, use either of the following queries: . http.request.referrer: \"https://example.com\" . http.request.referrer: https\\://example.com . You must escape following characters (unless surrounded by quotes/quoted search): ( ) : &lt; &gt; “ * { } . Smart Search: . Smart Search is a feature that enables searching without specifying the field. When using Smart Search, the system will identify fields that might be a possible match and search those fields for the given value. For example, if you search for “john white” without specifying a field, the system will search all fields that contain “john white” . To use Smart Search, enclose the search value in quotes. For example: . \"john white\". Numeric Range Search: . Elysium Search supports numeric searches, but there are a few considerations to keep in mind. Rounding: Elysium Search rounds numeric values to the nearest value that can be represented in double-precision floating-point format. This can affect the accuracy of numeric searches. Value will be rounded after 9 digits after the decimal point and values with a leading zero.. In cases where more precision is needed , users should use Double Quotes to treat the number as a string. 110.0 is treated as 110 and 10.00000 is treated as 10. Score : 123 . Score : “1.123456789123“ . Score : “10.0“ . Any number more than 16 digits (for both +ve and –ve numbers). We should use double quotes in that case. Adding a + in front treats the search value as a string. Hex number such as 0xabcd is not supported as a number. It must be searched as string. For example, to search for the value “01”, you would enter “01” in the search bar. You can also use the “&gt;” and “&lt;” operators to specify an open-ended range. For example, to search for documents where the “age” field is greater than 18, use the following: age &gt; 18 . To search for documents where the “age” field is less than 25, use the following: age &lt; 25 . Case Sensitivity: . Elysium Search is case-Sensitive by default, meaning that it does distinguish between uppercase and lowercase characters in searches. However, you can enable case-Insensitivity by using the following steps. | Open the Elysium Search. | Click on the three horizontal bars to open side menu. | Click on the “Discover“ button to open search. | On the search page find “settings icon” . | Click on the “settings icon” button. | Click on the “Case Sensitivity“ toggle to turn it on or off. | . Highlights: . Unquoted search will highlight the exact value entered while matching doing a partial match. User: Sannith → User: Sannith Reddy . Quoted search will highlight the exact value entered. User: “Sannith Reddy”: → User: Sannith Reddy . Wildcard matches will highlight depending on the *. User : *Reddy → User: Sannith Reddy . Limitations . Elysium Search has a number of limitations that users should be aware of: . | Arithmetic expressions and variables are not supported. | Only a limited set of character sets is supported. | Searches on timestamps are not available currently. | . Workarounds . Arithmetic Expressions: You will need to use an alternative approach if you need to perform calculations in your searches. One option is to you use a script to pre-process your data and add calculated fields,, which you can then search using Elysium Search or User can also use Dashboards(Looker). Copy/Paste: users have to be careful while copy pasting the results to search as there might be chances to characters which needs to be escaped or numbers with accurate precision. ",
    "url": "http://localhost:4000/documentation/search/index/#different-types-of-searches-you-can-use-to-filter-data",
    "relUrl": "/documentation/search/index/#different-types-of-searches-you-can-use-to-filter-data"
  }
}
